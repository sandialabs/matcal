{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Parameter uncertainty example - external noise and internal variability\nOnce again, for this example, the data comes from the same model as the one we are calibrating. \nIt is similar to the `Data Noise Issue - Low and High Noise Example`, \nbut demonstrates the two different Laplace approximation methods \navailable in MatCal.\n\nThe model is a bilinear function similar to elastic-plastic \nresponse with the parameter \"Y\" controlling where change from one linear \ntrend to the other takes place and the parameter \"H\" controlling the slope of the second trend. \nThe slope of the initial trend is fixed, as if it were known (as the elastic modulus typically would be).\nThe problem differs from the referenced example when we generate the data for the model.\nWe will use the model to generate data with uncertainty from two sources: noise and parameter uncertainty.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal import *\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After importing the tools from MatCal, \nwe create the model for this example.\nWe will use a PythonModel and define \nthe python function that will be the underlying\ncode driving the model.\nThe data will be fitted to a model with a bilinear\nresponse similar to an elastic-plastic model. \nThe initial slope (E) is assumed to be known. \nThe parameter \"Y\" determines where the model changes to the second linear trend \nThe parameter \"H\" determines the second slope.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def bilinear_model(**parameters):\n  max_strain = 1.0\n  npoints = 100\n  strain = np.linspace(0,max_strain,npoints)\n  E = 1.5 \n  Y = parameters['Y']\n  H = parameters['H']\n  eY = Y/E\n  stress = np.where(strain < eY, E*strain, Y + H*(strain-eY))\n  response = {'strain':strain, 'stress': stress}\n  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the function defined, we can now \ncreate the MatCal model and \nstudy parameters.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = PythonModel(bilinear_model)\nmodel.set_name(\"bilinear\")\nY = Parameter('Y',0.0,1.0, 0.501)\nH = Parameter('H',0.0,1.0, 0.501)\nparameters = ParameterCollection('parameters', Y, H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then use the model function directly to generate\nour data for the study.\nAs previously stated, we generate data that has both: (a) external \"measurement\" noise and \n(b) variability in the underlying response e.g. due to different processing of the same material.\nIn order to do so we need to import some more tools to support \nour random number generation\nand create a DataCollection with our generated data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\nfrom numpy.random import default_rng\n_rng = default_rng(seed=1234)\n\nexp_stddev = 0.005\nparameter_mean = [0.15,0.1]\nparameter_covariance = [[0.00005,0.0],[0.0,0.00005]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "generation for a given number of samples and\ngenerate data for a low number of samples \nas is typical with real experimental data.\nWe will repeat this later for more samples to \nshow the  :class:`matcal.core.parameter_studies.LaplaceStudy`\nconverges to the truth parameter covariance as the \nnumber of samples increases.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_data(n_samples):\n    data = DataCollection(\"noisy_data\")\n    parameter_samples = _rng.multivariate_normal(parameter_mean,\n                                                 parameter_covariance,n_samples)\n    for p in parameter_samples:\n        p_dict = {'Y':p[0],'H':p[1]}\n        response = bilinear_model(**p_dict)\n        d = copy.deepcopy(response)\n\n        d[\"stress\"] += exp_stddev*_rng.standard_normal(len(d[\"stress\"])) \n        data.add(convert_dictionary_to_data(d))\n    return data\ndata = generate_data(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we start with finding the least squares best fit parameters\nwith gradient-based calibration.\nWe create the calibration for our model parameters, \nand define an objective for fitting the model response to the data.\nOnce again, we put this in a function so that it can be re-used later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "objective = CurveBasedInterpolatedObjective('strain','stress')\nobjective.set_name(\"objective\")\ndef get_best_fit(data):\n    calibration = GradientCalibrationStudy(parameters)\n    calibration.add_evaluation_set(model, objective, data)\n    results = calibration.launch()\n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we run the calibration and store the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = get_best_fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running the calibration, we load the optimal parameters and the best fit\nmode response. \nNote the best parameters are close but not precisely the mean parameters\nused to generate the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_name = data.state_names[0]\nbest_parameters = results.best.to_dict()\nbest_response = results.best_simulation_data(model.name, state_name)\n\nprint(\"Best fit parameters:\")\nprint(results.best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now compare the model (lines) response to the data (red lines with points). \nIn the eyeball norm the best fit model is in the middle of the noisy data,\ni.e. there are about as  many points above the fitted line as below. \nThis is due to using a mean squared error objective. \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\ndef compare_data_and_model(data,model_response,fig_label=\"\"):\n  for c in data.values():\n    for i,d in enumerate(c):\n      d = convert_data_to_dictionary(d)\n      plt.figure(fig_label)\n      label = None\n      if i==0:\n        label = \"data\"\n      plt.plot(d['strain'],d['stress'],'-ro',ms=3,label=label)\n  for i,response in enumerate(model_response):\n    plt.figure(fig_label)\n    label = None\n    if i==0:\n      label=\"model\"\n    plt.plot(response['strain'], response['stress'], label=label)\n  \n  plt.figure(fig_label)\n  plt.xlabel(\"STRAIN\")\n  plt.ylabel(\"STRESS\")\n  plt.legend()\n  plt.show()\n\ncompare_data_and_model(data,[best_response],fig_label=\"best fit calibration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we investigate the effect of noise on the parameter uncertainty using MatCal's\n:class:`~matcal.core.parameter_studies.ClassicLaplaceStudy`\nfor (inexpensive, approximate) uncertainty quantification. \nThe LaplaceStudy uses the Laplace approximation to calibrate \na mutlivariate normal distribution characterizing the parameter uncertainty. \nSince it uses the parameter hessian near the optimum parameters \nit is an approximation of the parameter uncertainty obtained by more expensive methods.\nWe use the same data and give the Laplace study the optimal parameters to use\nas a center for the finite difference evaluation of the Hessian.\nAfter initial setup, we can run the uncertainty quantification studies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "laplace = ClassicLaplaceStudy(parameters)\nlaplace.add_evaluation_set(model, objective, data)\nlaplace.set_parameter_center(**best_parameters)\nlaplace_results_external = laplace.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When they finish, we can sample the resulting distributions\nusing :func:`matcal.core.parameter_studies.sample_multivariate_normal`.\nWe request 50 samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_samples = 50\nnoisy_parameters = sample_multivariate_normal(num_samples, laplace_results_external.mean.to_list(), \n                                              laplace_results_external.estimated_parameter_covariance, \n                                              param_names=laplace_results_external.parameter_order)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each sample, we run the parameters through \nthe model using a :class:`matcal.core.parameter_studies.ParameterStudy`\nso that we can compare the uncertain model to the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def push_forward_parameter_samples(parameters,samples):\n  param_study = ParameterStudy(parameters)\n  param_study.add_evaluation_set(model, objective, data)\n  for Y_samp, H_samp in zip(samples[\"Y\"], samples[\"H\"]):\n    param_study.add_parameter_evaluation(Y=Y_samp, H=H_samp)\n  results = param_study.launch()\n  responses = []\n  for i in range(num_samples):\n    response = results.simulation_history[model.name][state_name][i]\n    responses.append(response)\n  return responses\nmodel_response = push_forward_parameter_samples(parameters, noisy_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we plot a comparison of the model responses and the calibration data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "compare_data_and_model(data,model_response,\n                       fig_label=\"model response samples from external noise assumption\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note how the data is not encapsulated by the estimated uncertainty. This \nis because there are many points included in the calibration and \nthe uncertainty due to noise is small.\n\nNext We compare the estimated parameter covariance \nwith the known covariance using a percent error \nmeasure and see that the error is quite high as expected. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compare_laplace_results(estimated_covar):\n    print(\"Estimated covariance:\")\n    print(estimated_covar)\n    print(\"\\n\")\n    print(\"Actual covariance:\")\n    print(parameter_covariance)\n    print(\"\\n\")\n    print(\"% Error:\", (np.mean(estimated_covar-parameter_covariance)\n                       /np.mean(parameter_covariance)*100))\n    print(\"\\n\")\n\n\ncompare_laplace_results(laplace_results_external.estimated_parameter_covariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now examine the parameter distribution when the error is attributed \nto model form error as is done with the :class:`~matcal.core.parameter_studies.LaplaceStudy`\nHere the variability is associated with variability of the model parameters. \nFor this example the parameter distribution tends to cover the calibration data, \nbut the quality of the estimate is dependent on the number of data samples \nincluded and how well the data conform to a multivariate normal.\nWe perform the same plotting and percent error calculation as was done \nfor the .\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def estimate_uncertainty_due_model_error(data):\n    laplace = LaplaceStudy(parameters)\n    laplace.add_evaluation_set(model, objective, data)\n    laplace.set_parameter_center(**best_parameters)\n    laplace.set_noise_estimate(exp_stddev)\n    laplace_results_internal = laplace.launch()\n    print(\"Mean\",laplace_results_internal.mean.to_list())\n    print(\"Covar\",laplace_results_internal.estimated_parameter_covariance)\n    print(\"Param order\", laplace_results_internal.parameter_order)\n    uncertain_parameters = sample_multivariate_normal(num_samples, \n                                                      laplace_results_internal.mean.to_list(), \n                                                      laplace_results_internal.estimated_parameter_covariance, \n                                                       param_names=laplace_results_internal.parameter_order) \n    model_response = push_forward_parameter_samples(parameters, uncertain_parameters)\n    compare_data_and_model(data,model_response,fig_label=\"model response samples from internal variability assumption\")\n    compare_laplace_results(laplace_results_internal.estimated_parameter_covariance)\n\nestimate_uncertainty_due_model_error(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note how the data is now encapsulated by the estimated uncertainty. This \nis because the error is now appropriately \nattributed to uncertainty in the model. \nAlso, the estimate for the parameter covariance has improved, although \nit is still higher than we would like for verification. \n\nNext, we redo the study with \nmore sample and see the effect it has \non the predicted parameter uncertainty.. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = generate_data(200)\nresults = get_best_fit(data)\nbest_parameters = results.best.to_dict()\nestimate_uncertainty_due_model_error(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 50 samples propagated through the model have less spread. \nThis is because the uncertainty attributed to parameter \nuncertainty is better accounted for with the larger sample size.\nAlso, the percent error metric is much improved showing that the result \nis approaching the expected value. \n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}