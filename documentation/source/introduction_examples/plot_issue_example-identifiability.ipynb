{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Model Identifiability Issue Example\nIn this example the underlying data model is linear, \nand the fitted model is over-parameterized i.e., is too complex for the data. \nThere are too many parameters to\nfit the data uniquely, and this is not due to the number of data points.\nIt is due to the form of the model (it is bilinear) vs. the trend in the data (it is just linear).\nSince \nthe error contours are not closed,\nthere is no unique best fit as the calibration is formulated.\nThe data should indicate that H = E, but nothing informs what Y should be. \nAlthough simplistic, this case is emblematic of the calibration of some material models. \nMore complex cases with nonlinearities can exhibit multiple wells with multiple optima.\nThere are ways of combating this such as L1 regularization of the parameters that add \na small cost to calibrating non-zero parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal import *\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a python function to be used as a MatCal PythonModel\nfor this calibration. The python model uses Numpy in the function\nand requires that Numpy be imported within the function.\nAs stated above, the data will be fitted to a model with a \nbilinear response similar to an elastic-plastic model.\nThe initial slope (E) is assumed to be known. \nThe parameter \"Y\" determines where the model changes to \nthe second linear trend and the parameters \"H\" determines the second slope.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def bilinear_model(Y, H, npoints=100):\n  import numpy as np\n  max_strain = 1.0\n  strain = np.linspace(0,max_strain,npoints)\n  E = 1.5 \n  eY = Y/E\n  stress = np.where(strain < eY, E*strain, Y + H*(strain-eY))\n  response = {'strain':strain, 'stress': stress}\n  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the function for the model defined above,\nwe create the parameters for our MatCal \ncalibration study and the MatCal PythonModel\nfrom the function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y = Parameter('Y',0.0,1.0, 0.5)\nH = Parameter('H',0.0,2.0, 0.5)\nparameters = ParameterCollection('parameters', Y, H)\nmodel = PythonModel(bilinear_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A simple gradient-based calibration with\na mean squared error objective is enough \nto illustrate the point. We create the data, \ncreate the calibration based on the model parameters, \nand define an objective for fitting the model response to the data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nexp_points=30\ndef generate_data(stddev):\n    data = bilinear_model(Y=0.5, H=1.5, npoints=nexp_points)\n    from numpy.random import default_rng\n    _rng = default_rng(seed=12345)\n    data['stress'] += stddev*_rng.standard_normal(len(data[\"stress\"])) \n    data = convert_dictionary_to_data(data)\n    return data\ndata = generate_data(stddev=0)\ncalibration = GradientCalibrationStudy(parameters)\nobjective = CurveBasedInterpolatedObjective('strain','stress')\ncalibration.add_evaluation_set(model, objective, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then run the calibration and select the optimal parameters and the best fit.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = calibration.launch()\nbest_parameters = results.best.to_dict()\nbest_response = results.best_simulation_data(model, 'matcal_default_state')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, compare the response curves.\nWe plot the calibrated model with lines and the data with points. \nWe can see that an acceptable fit has been found. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nplt.figure()\nplt.plot(best_response['strain'],best_response['stress'],'b',label=\"fit\")\nplt.scatter(data['strain'],data['stress'],2,'r',label=\"data\")\nplt.xlabel(\"STRAIN\")\nplt.ylabel(\"STRESS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, we know that the parameter Y should have no effect. \nNext, examine how the error changes in the vicinity of the best fit.\nThis a helper function to evaluate the error on a grid of \nparameter values for plotting. MatCal's ParameterStudy can also do this.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def sample_error(model, data, ranges = {\"Y\":[0.0,1.0],\"H\":[0.0,2.0]} ):\n  Ys, Hs = np.mgrid[ ranges[\"Y\"][0]:ranges[\"Y\"][1]:100j, ranges[\"H\"][0]:ranges[\"H\"][1]:100j]\n  Zs = np.empty_like(Ys)\n  for i in range(Ys.shape[0]):\n    for j in range(Ys.shape[1]):\n      parameters = {\"Y\":Ys[i,j],\"H\":Hs[i,j]}\n      response = model(**parameters, npoints=nexp_points)\n      residual = response['stress']-data['stress']\n      error = np.sum(residual**2)\n      Zs[i,j] = error\n  return Ys,Hs,Zs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The contour plot depicts the change in error. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Ys,Hs,Zs = sample_error(bilinear_model,data)\nplt.figure()\nplt.contourf(Ys,Hs,np.log(Zs),20)\nplt.grid(True)\nplt.xlabel(\"Y\")\nplt.ylabel(\"H\")\nplt.colorbar(label=\"log error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that there are no closed contours, and so, no optima. \nAny Y value with the correct H value gives equivalent errors. \nThe problem is not \"well-posed\" as we constructed it. \nRemoving or setting the \"Y\" parameter is the easiest fix; \nhowever, with more complex models this issue \nand its remedy might not be as apparent.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we run the calibration again with a new initial point,\nwe can again get a calibrated result. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parameters.update_parameters(Y=0.1, H=0.3)\ncalibration = GradientCalibrationStudy(parameters)\nobjective = CurveBasedInterpolatedObjective('strain','stress')\ncalibration.add_evaluation_set(model, objective, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we run the calibration, we inspect the new results and see\nthat a different value of Y has been provided.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = calibration.launch()\nbest_parameters_different_start = results.best.to_dict()\nprint(\"Initial best:\", best_parameters)\nprint(\"Updated best:\", best_parameters_different_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One way of determining the identifiability of the parameters \nis to determine the curvature of the objective around the \nfound minima. If the curvature is low or zero, the parameter \ncannot be well identified at least with the current objective(s).\nThe objective function curvature can be obtained using our \n:class:`~matcal.core.parameter_studies.ClassicLaplaceStudy`\nwhich calculates the hessian of objective function around a point\nin the objective space.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The hessian is approximated using finite differencing which can be very expensive\n    for models will large numbers of parameters.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Our finite differencing currently does not account for parameter bounds. \n   Errors may result, so update the center such that parameter values are not on the bounds.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "study = ClassicLaplaceStudy(Y, H)\nbest_H = best_parameters_different_start[\"H\"]\nbest_Y = best_parameters_different_start[\"Y\"]\nstudy.set_parameter_center(H=best_H, Y=best_Y-1e-8)\nstudy.set_step_size(1e-8)\nstudy.add_evaluation_set(model, objective, data)\nstudy.run_in_serial()\nresults = study.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running the study, we inspect the hessian and see that \nthe hessian with respect to Y is essentially zero at this point.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(results.parameter_order)\nprint(results.hessian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, for real data with model form error and noise it might not be \nso apparent. If we re-generate data with noise and re-run the \n:class:`~matcal.core.parameter_studies.ClassicLaplaceStudy`,\nwe can see the hessian of the objective \nwith respect to Y is now non-zero, but small.\nWe have to find the new minimum before evaluating at the hessian.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = generate_data(0.05)\nparameters.update_parameters(Y=0.25, H=0.25)\ncalibration = GradientCalibrationStudy(parameters)\nobjective = CurveBasedInterpolatedObjective('strain','stress')\ncalibration.add_evaluation_set(model, objective, data)\nresults = calibration.launch()\nbest_parameters = results.best.to_dict()\nbest_response = results.best_simulation_data(model, 'matcal_default_state')\n\nplt.figure()\nplt.plot(best_response[\"strain\"],best_response[\"stress\"],'b',label=\"fit\")\nplt.scatter(data[\"strain\"],data[\"stress\"],2,'r',label=\"data\")\nplt.xlabel(\"STRAIN\")\nplt.ylabel(\"STRESS\")\nprint(best_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the new optimum found, we run the \n:class:`~matcal.core.parameter_studies.ClassicLaplaceStudy`\nand inspect the new results. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "study = ClassicLaplaceStudy(Y, H)\nbest_H = best_parameters[\"H\"]\nbest_Y = best_parameters[\"Y\"]\nstudy.set_parameter_center(H=best_H, Y=best_Y+3e-8)\nstudy.set_step_size(1e-8)\nstudy.add_evaluation_set(model, objective, data)\nstudy.run_in_serial()\nresults = study.launch()\nprint(results.parameter_order)\nprint(results.hessian)\nprint(np.linalg.eig(results.hessian))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see\nthat hessian of the objective with respect to Y is no longer zero, \nbut it is lower than the \nhessian with respect to H. This indicates\nY may not be well identified by the objective. However, it is not \nimmediately obvious that it cannot be identified.\nThe hessian now has non-zero \neigen values and is positive definite, indicating \nthe model might have a local minima at the calibrated\npoint. Noise can create a minimum or local minima in cases such as \nthe one shown here.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}