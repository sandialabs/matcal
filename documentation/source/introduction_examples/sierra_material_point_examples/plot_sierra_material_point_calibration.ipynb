{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Successful Calibration\n\nAs stated previously, we present the calibration of our \n:class:`~matcal.sierra.models.UniaxialLoadingMaterialPointModel`\nto uniaxial compression data for a 6061-T6 aluminum from the Ductile Failure Project at Sandia\n:cite:p:`DE_L2_Ductile_Failure`. This example and calibration will\nconsist of three steps:\n\n#. Data overview, analysis and preprocessing.\n#. Model selection and preparation.\n#. Calibration execution and results review.\n \nTo begin, import all of MatCal's calibration tools:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 5\n\nfrom matcal import *\nimport matplotlib.pyplot as plt\nplt.rc('text', usetex=True)\nplt.rc('font', family='serif')\nplt.rc('font', size=12)\nfigsize = (4,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we will review the data for model calibration. For the \n:class:`~matcal.sierra.models.UniaxialLoadingMaterialPointModel`, \nwe support only specific data fields for calibration. These are \n'time', 'engineering_strain', 'engineering_stress', 'true_strain', 'true_stress' and 'contraction'. \nTo learn more see `Uniaxial Loading Material Point Model`. For this calibration,\nwe will be using true stress and strain data for the calibration. This data is used for \nboth the objective calculation and boundary condition generation. To load the data, \nwe can use the :class:`~matcal.core.data_importer.BatchDataImporter` tool that imports \ndata from multiple files. It puts the data into a :class:`~matcal.core.data.DataCollection`\nwhich has a basic plotting method :meth:`~matcal.core.data.DataCollection.plot` for\nsimple visualization and debugging.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_collection = BatchDataImporter(\"uniaxial_material_point_data/*.csv\").batch\ndata_collection.plot(\"true_strain\", \"true_stress\")\ndata_collection.plot(\"time\", \"true_strain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is clear from the data that the test specimens' 'time' data fields\ndo not have a common start time. Although this is not necessarily\nan issue for MatCal's :class:`~matcal.sierra.models.UniaxialLoadingMaterialPointModel`, \nit does makes visualizing the data inconvenient. Since the :class:`~matcal.core.data.Data` class\nis derived from NumPy arrays :cite:p:`harris2020array`, it is easy to modify the data for convenient viewing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for state_data_list in data_collection.values():\n  for data in state_data_list:\n    data['time'] = data['time'] - data['time'][0]\ndata_collection.plot(\"time\", \"true_strain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the updated plots, two features are evident: \n\n  #. Dataset C-RD-01 appears to have gone unstable in some fashion and \n  #. Dataset C-ST-01 has a period of unloading. \n\nThese features are important to take note of due to how\nMatCal will produce boundary conditions for its :class:`~matcal.sierra.models.UniaxialLoadingMaterialPointModel`. \nThe :class:`~matcal.sierra.models.UniaxialLoadingMaterialPointModel` has \na method :meth:`~matcal.sierra.models.UniaxialLoadingMaterialPointModel.add_boundary_condition_data`\nthat is used to provide data for boundary condition determination for the model. \nThis boundary condition determination is done by state since maximum deformation, \nmaterial behavior and experiment setup can vary significantly over different states. \nThese boundary conditions are determined from the data according to the following:\n\n  #. Find the data in each state with the largest strain. This dataset will be used\n     to produce the boundary condition function.\n  #. If 'time' and 'engineering_strain' or 'true_strain' data exists for \n     the chosen dataset, use this as the direct input strain function for the model.\n     The model currently only uses engineering strain input, so true strain data \n     is converted to engineering strain which is then applied to the model as \n     an appropriately scaled displacement function. \n  #. If 'engineering_strain' and 'true_strain' are fields for the data set,\n     use the 'engineering_strain' field for the boundary condition. Otherwise,\n     if only 'true_strain' is available, convert it to engineering strain and\n     use it for the boundary condition. \n  #. If 'time' is not in the data, but the state has a state variable named \n     'engineering_strain_rate'. Apply engineering strain linearly at the given \n     state engineering strain rate until the model has reached the maximum strain measured \n     for that state. \n  #. If 'time' is not in the data and no 'engineering_strain_rate' state variable is present,\n     deform the model from no strain to the maximum strain over 1 second.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>If *both* true strain and \n    engineering strain exist in the data, it will default to using the \n    engineering strain data to create the boundary condition. As a result,\n    any changes applied to the true strain data in an effort to modify \n    the model boundary conditions should also be done to the engineering strain data. \n    In most cases, if modifying the true strain data for boundary condition purposes, it\n    is best to remove the engineering strain from the data if both are present in the data\n    to begin with. MatCal will automatically generate an engineering strain data field\n    from the true strain data field.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The :class:`~matcal.core.data.Data` or :class:`~matcal.core.data.DataCollection` used for boundary \n          condition generation does not need to be the same as that use for calibration. As a result, \n          custom boundary condition data can be generated by the user for more complex load cases. See \n          :func:`~matcal.core.data.convert_dictionary_to_data`.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Compression boundary conditions are supported and must be passed as negative strain values to the model.\n          If compression is used, the model will output negative stresses. If compression data is provided\n          from the source with positive stress/strain values use :func:`~matcal.core.data.scale_data_collection` to convert\n          the data to negative stress/strain.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on this information, we will choose to force the dataset C-ST-01 to be used as the data\nfor boundary condition generation.\nTo do so, we will create a new data class that consists of a NumPy view\ninto a subset of the dataset. We do this by first selecting the dataset from our \n:class:`~matcal.core.data.DataCollection` which is indexed\nfirst by :class:`~matcal.core.state.State` or :meth:`~matcal.core.state.State.name`.\nand the order in which the data was added to the data collection.\nIn this case no state is defined, so the default state name \"matcal_default_state\" is used. \nThe data are then added to the :class:`~matcal.core.data.DataCollection` by sorting based on the \nfilename, so we will select the data at index location 1.\nNext, we use NumPy array slicing to manipulate the data and feed only the data that are required to the \nmodel for boundary condition generation. In this case the model only needs the engineering strain field\nfrom the data of choice since we do not need to simulate the loading history with this model form. When \nonly the engineering strain data is provided for boundary condition generation, the model will be deformed\nfrom no deformation to the maximum strain in the data for the state of interest in 1 second.\nFinally, since this data was taken in compression, we need to convert the data to negative strains\nso that it is interpreted correctly during boundary condition generation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "boundary_data = data_collection[\"matcal_default_state\"][1]\nboundary_data = boundary_data[[\"engineering_strain\"]]\nboundary_data.set_name(\"dataset 1 derived BC data\")\nboundary_data_collection = DataCollection('boundary_data', boundary_data)\nboundary_data_collection = scale_data_collection(boundary_data_collection, \"engineering_strain\", -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>With the current model form, the model will produce the same result\n  whether in compression or tension as long as the proper boundary condition \n  is produced. The data is converted to compression \n  here to demonstrate that compression data can be used to create compressive\n  models and, since we a working with engineering strains, compression is required.\n  Correctly modeling compressive or tensile stress states \n  is required for models with tension/compression asymmetry, \n  and is considered good practice for all cases.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_collection = scale_data_collection(data_collection, \"true_strain\", -1)\ndata_collection = scale_data_collection(data_collection, \"true_stress\", -1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the boundary condition data chosen, we can now analyze the data\nto choose a model form for calibration. The data show\nthat after yield the material hardens before the hardening rate reduces and \neventually a saturation stress is reached. As a result, we choose to calibrate \na J2 plasticity model with Voce hardening to the material model which should match the data well.\nThe flow rule is defined by:\n\n\\begin{align}\\sigma_f = Y + A\\left[1-\\exp\\left(-b\\varepsilon\\right)\\right]\\end{align}\n\nwhere $Y$ is the material yield, $A$ is the Voce hardening modulus, $b$\nis the Voce exponent, and $\\varepsilon$ is the material plastic strain. As with \nany plasticity model, when the flow\nstress is greater than the equivalent stress, which is the von Mises stress for this material,\nplastic flow occurs. We will need to calibrate the $Y$, $A$, and $b$ parameters. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y = Parameter('Y', 30, 60, 50)\nA = Parameter('A', 1, 500, 100)\nb = Parameter('b', 5, 30, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create a :class:`~matcal.sierra.material.Material` class \nand corresponding material file for the calibration. \nThe input deck for this material model in SIERRA/SM is shown below::\n\n   begin material j2_voce\n     density = 0.000254\n     begin parameters for model j2_plasticity\n       youngs modulus                = 9.9e6\n       poissons ratio                =   0.33\n       yield stress                  = {Y*1e3}\n\n       hardening model = voce\n       hardening modulus = {A*1e3}\n       exponential coefficient = {b} \n      end\n   end\n\nThe material that we are calibrating is a 6061-T6 aluminum. The elastic\nproperties and density can be pulled from the literature. In this case\nwe use values provided by MMPDS10 :cite:p:`MMPDS10`. \nWith this SIERRA/SM input saved in the current directory as \"sierra_sm_voce_hardening.inc\", \nwe can create the :class:`~matcal.sierra.material.Material` and the \n:class:`~matcal.sierra.models.UniaxialLoadingMaterialPointModel`. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "j2_voce = Material(\"j2_voce\", \"sierra_sm_voce_hardening.inc\", \"j2_plasticity\")\n\nmat_point_model = UniaxialLoadingMaterialPointModel(j2_voce)\nmat_point_model.add_boundary_condition_data(boundary_data_collection)\nmat_point_model.set_name(\"compression_mat_point\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next the parameters are passed to a study. In this case, we will \nuse a :class:`~matcal.dakota.local_calibration_studies.GradientCalibrationStudy`\nto perform the calibration. For this simple set of data and simple model, \nthis type of study will work well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibration = GradientCalibrationStudy(Y, A, b)\ncalibration.set_results_storage_options(results_save_frequency=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last component needed for the calibration is an objective to minimize.\nFor this calibration, we will use a \n:class:`~matcal.core.objective.CurveBasedInterpolatedObjective`\nthat matches the true stress/strain curve generated using the model to the \nexperimental data collected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "objective = CurveBasedInterpolatedObjective('true_strain','true_stress')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>The :class:`~matcal.core.objective.CurveBasedInterpolatedObjective` expects\n    the independent data field to be monotonically increasing since it is uses\n    the NumPy interp method to interpolate the simulation data to the experiment\n    data independent field locations. To support negative data, MatCal \n    sorts the data so that the independent variable is monotonically increasing \n    to meet this requirement. Be sure your data will behave as intended when passed\n    to this objective.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One more step remains before this objective is ready for use in the calibration.\nSince the material data being used for the calibration has unloading data \nand our  model does not, we must modify the objective or the data to remove this data \nfrom the calibration. With the objective we are using, we do not want to modify the \nQoI Extractor since this objective has a predefined extractor for interpolation. \nWe also want to keep the entire original dataset. This leaves us with the \noption to use a weighting function that modifies the residuals such that \nthe unloading points do not affect the objective. We also *should* remove the \nelastic loading portion of the curve. Since we are not \ncalibrating the elastic parameters, it should not contribute to the residual. \nFurthermore, since this portion of the curve is steep, even small errors in the slope could\nlead to large contribution \nto the objectives. Therefore, to ensure the objective provides the calibration we want, we use a\n:class:`~matcal.core.residuals.UserFunctionWeighting` to ensure only the data \nwe want to use for calibration affects the objective. \nTo do so, we define a function that performs the residual weighting. \nOnce again, we can leverage NumPy array slicing to select the data \nwe wish to exclude and set their weights to zero, effectively removing their\ninfluence on the objective.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def remove_high_and_low_strain_from_residual(true_strains, true_stresses, residuals):\n    import numpy as np\n    weights = np.ones(len(residuals))\n    weights[(-true_strains > 0.5) | (-true_strains < 0.0035)] = 0\n    return weights*residuals\n\nresidual_weights = UserFunctionWeighting(\"true_strain\", \"true_stress\", \n                                         remove_high_and_low_strain_from_residual)\n\nobjective.set_field_weights(residual_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To learn more about :class:`matcal.core.residuals.UserFunctionWeighting` please view\nits documentation. With the model, objective and data defined, we can now give the study \nan evaluation set. These evaluation sets give the study all pieces needed to evaluate\nan objective essentially tying a dataset, model and objective together for evaluation.\nAlthough multiple evaluation sets can be added to a study, only one is needed for this basic\ncalibration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibration.add_evaluation_set(mat_point_model, objective, data_collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last step is to launch the calibration study and review the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibration.set_core_limit(4)\nresults = calibration.launch()\nprint(results.best.to_dict())\nmake_standard_plots(\"true_strain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The calibration completes with the Dakota output::\n\n  ***** RELATIVE FUNCTION CONVERGENCE *****\n\nindicating that the algorithm completed successfully. From \nthe plots it is clear that the model matches the experimental \ndata well, and the final objective function value of around 0.00692 \nalso indicates a quality calibration with low model form error.\nSince this is a calibration \nto true stress/strain data, it is also straight forward to verify the fit\nanalytically. From the QoI plot, we can see yield is around 42 ksi and the \nsaturation stress is around 55 ksi which agrees with the calibrated parameters of\n$Y = 42.17 \\text{ ksi}$ and $A = 12.83 \\text{ ksi}$.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}