{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Displacement Calibration Verification -  First Attempt\nIn this example, we attempt to calibrate \nthe five parameters of our verification \nproblem using only the load-displacement curve.\nSince the `Objective Sensitivity Study`\nshows that the objective is at a minimum  \nit should be possible. However,\nsince the model is fairly expensive \nwe attempt to do so using a gradient method. \nSpecifically, we use the \n:class:`~matcal.dakota.local_calibration_studies.GradientCalibrationStudy`\nusing Dakota's ``nl2sol``\nmethod implementation.\nAs we will see, the objective is difficult to \ncalibrate due to the observed discontinuities and\nlikely local minima throughout the parameter space. \nAs a result, the method fails with little progress.\n\nTo begin we import the MatCal tools necessary for this study\nand import the data that will be used for the calibration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal import *\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rc('text', usetex=True)\nplt.rc('font', family='serif')\nplt.rcParams.update({'font.size': 12})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we import the data\nwe wish to use in the study. \nFor this study, we import \nthe Exodus data from the \n``0_degree`` synthetic data set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "synthetic_data = FieldSeriesData(\"../../../docs_support_files/synthetic_surf_results_0_degree.e\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After importing the data, we\nselect the data we want for our study.\nFor the load-displacement curve objective, \nwe want all time steps up to 92.5% of peak load \npast peak load. These data are selected \nfor the ``synthetic_data`` object below\nusing NumPy array slicing and tools. \nWe do this because we only run the simulation \nuntil its load has dropped to 92.5% of peak load after peak load.\nAs stated previously, this is done for model robustness\nand to reduce simulation time. For certain \nparameters in the available parameter space, \npeak load will occur early in the displacement space \nand the model will not be able to run to the \nexpected displacement. With adaptive time stepping, \nthe model will run for an extended period without significant progress\nand use up valuable resources. We force the model to exit\nto avoid this. The discontinuity this introduces \nis unavoidable as the model cannot run successfully \nfor any set of input parameters.  \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "peak_load_arg = np.argmax(synthetic_data[\"load\"])\n\ndesired_arg = np.argmin(np.abs(synthetic_data[\"load\"]\\\n                               [peak_load_arg:]-np.max(synthetic_data[\"load\"])*0.925))\nsynthetic_data = synthetic_data[:desired_arg+1+peak_load_arg]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the data imported and selected, \nwe plot the data to verify our \ndata manipulation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dc = DataCollection(\"data\", synthetic_data)\ndc.plot(\"displacement\", \"load\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After importing and preparing the data, \nwe create the model that will be used \nto simulate the characterization test. \nWe will use a :class:`~matcal.sierra.models.UserDefinedSierraModel`\nfor this example. We setup the model input to require\nan external \nSierraSM material model input file. We create it \nnext using python string and file tools.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mat_file_string = \"\"\"begin material test_material\n  density = 1\n  begin parameters for model hill_plasticity\n    youngs modulus  = {elastic_modulus*1e9}\n    poissons ratio  = {poissons}\n    yield_stress    = {yield_stress*1e6}\n\n    hardening model = voce\n    hardening modulus = {A*1e6}\n    exponential coefficient = {n}\n\n    coordinate system = rectangular_coordinate_system\n    \n    R11 = {R11}\n    R22 = {R22}\n    R33 = {R33}\n    R12 = {R12}\n    R23 = {R23}\n    R31 = {R31}\n  end\nend\n\"\"\"\n\nwith open(\"modular_plasticity.inc\", 'w') as fn:\n    fn.write(mat_file_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the material file created, \nthe model can be instantiated. \nWe provide the :class:`~matcal.sierra.models.UserDefinedSierraModel`\nwith the correct user supplied \ninput deck and mesh. For this model, \nwe use ``adagio`` as the simulation\nsolid mechanics code. Next, we use the appropriate model \nmethods to setup the model for the study.\nMost importantly we pass the correct \nmodel constants to it and provide the model \nwith the correct results model output \ninformation. The model constants\npassed to the model are the uncalibrated parameters\ndescribed in `Full-field Verification Problem Material Model`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = UserDefinedSierraModel(\"adagio\", \"synthetic_data_files/test_model_input_reduced_output.i\", \n                               \"synthetic_data_files/test_mesh.g\", \"modular_plasticity.inc\")\nmodel.set_name(\"test_model\")\nmodel.add_constants(elastic_modulus=200, poissons=0.27, R22=1.0, R33=0.9, R23=1.0, R31=1.0)\nmodel.read_full_field_data(\"surf_results.e\")\nfrom site_matcal.sandia.computing_platforms import is_sandia_cluster, get_sandia_computing_platform\nfrom site_matcal.sandia.tests.utilities import MATCAL_WCID\nnum_cores=96\nif is_sandia_cluster():\n    platform = get_sandia_computing_platform()\n    num_cores = platform.get_processors_per_node()\n    model.run_in_queue(MATCAL_WCID, 0.5)\n    model.continue_when_simulation_fails()\nmodel.set_number_of_cores(num_cores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create the objective that will \nbe used for the calibration. \nThe independent variable is the \"displacement\"\nand the calibration residual is determined from \nthe \"load\" result. The ``right=0`` informs \nthe objective to provide a zero value for loads\nif it is forced to extrapolate. This occurs when \nthe simulation plastically localizes and exits\nbefore its displacement reaches the maximum displacement\nof the synthetic data. It contributes to the observed\nobjective discontinuity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "load_objective = CurveBasedInterpolatedObjective(\"displacement\", \"load\", right=0)\nload_objective.set_name(\"load_objective\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create the material model \ninput parameters for the study. We provide \nrealistic bounds that one may expect \nfor an austenitic stainless steel based\non our experience with the material. \nThis results in an initial point far from \nthe true values used for the synthetic data generation\nand is a stressing test for a local \ngradient based method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y = Parameter(\"yield_stress\", 100, 500.0)\nA = Parameter(\"A\", 100, 4000)\nn = Parameter(\"n\", 1, 10)\nR11 = Parameter(\"R11\", 0.8, 1.1)\nR12 = Parameter(\"R12\", 0.8, 1.1)\n\nparam_collection = ParameterCollection(\"Hill48 in-plane\", Y, A, n, R11, R12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create the calibration \nstudy and pass the parameters \nrelevant to the study during its\ninitialization. We then set \nthe total cores it can use locally and\npass the data, model and objective to \nit as an evaluation set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "study = GradientCalibrationStudy(param_collection)\nstudy.set_results_storage_options(results_save_frequency=len(param_collection)+1)\nstudy.set_core_limit(100)\nstudy.add_evaluation_set(model, load_objective, synthetic_data)\nstudy.set_working_directory(\"load_disp_cal_initial\", remove_existing=True)\nstudy.set_step_size(1e-4)\nstudy.do_not_save_evaluation_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we launch the study save the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = study.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the study completes, \nwe extract the calibrated parameters \nand evaluate the error.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibrated_params = results.best.to_dict()\nprint(calibrated_params)\n\ngoal_results = {\"yield_stress\":200,\n                \"A\":1500,\n                \"n\":2,\n                \"R11\":0.95, \n                \"R12\":0.85}\n\ndef pe(result, goal):\n    return (result-goal)/goal*100\n\nfor param in goal_results.keys():\n    print(f\"Parameter {param} error: {pe(calibrated_params[param], goal_results[param])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These error's are much higher \nthan desired for a successful calibration. \nThis is expected as the problem was \ndesigned to have non-unique solutions \nwhen calibrating only to the load-displacement curves.\nUsing MatCal's standard plot, we can \nsee that the load-displacement curve \nmatches quite well. In the follow-on, \nexamples we will show how adding full-field \ndata improves results and how the different \nfull-field methods perform.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\ninit_dir = os.getcwd()\nos.chdir(\"load_disp_cal_initial\")\nmake_standard_plots(\"displacement\")\nos.chdir(init_dir)\n\n# sphinx_gallery_thumbnail_number = 2"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}