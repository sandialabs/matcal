{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full-field Interpolation Verification\nIn this example, we use an analytical function \nto test and verify our use and implementation of the \nCompadre GMLS algorithm. \nWe design the verification problem to be \nrepresentative of the full-field data \nwe will primarily use the method on for \ndata interpolation and extrapolation to \npoints relatively close to the measurement locations. \nWith this in mind, we perform the following using a two dimensional function\nas our test function:\n\n#.  We evaluate the function on a set of points over a\n    a domain that is 5% smaller then the domain we \n    will interpolate and extrapolate to. This will \n    be referred to as our measurement grid and is\n    meant to be representative of experimental data.\n#.  We add noise with a normal distribution to the \n    data generated in the previous step. The noise \n    has a maximum amplitude of 2.5% of the function \n    maximum value to represent the noise present \n    in measured data.\n#.  We create a separate domain with 75% of the points \n    from the measured grid that is 5% larger\n    in both the X and Y directions and evaluate the function \n    at these points without noise. This is \n    to be used as the truth \n    value of the function and this set of \n    points will be referred to as the simulation \n    grid. We will attempt to reproduce \n    these values with GMLS \n    interpolation and extrapolation.\n#.  We loop over different input options to the GMLS \n    algorithm and evaluate the accuracy of the method\n    against the truth data with three measures of error:\n    (1) the maximum percent error of the field produced \n    by the GMLS tool, (2) the a normalized L2 norm of this \n    field and (3) plots of the error field for all of the \n    input options studied for the GMLS algorithm.\n\nTo begin we import the libraries and tools we will be using to perform this study.\n\n# sphinx_gallery_thumbnail_number = 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal import *\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#plt.rc('text', usetex=True)\n#plt.rc('font', family='serif')\n#plt.rcParams.update({'font.size': 12})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we defined the domain for our measured grid using numpy tools.\nThe domain is about 15 mm high (6 inches) and 7.6 mm wide (3 inches). \nThe measured grid has 400 points in each dimension (x, y). \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "H = 6*0.0254\nW = 3*0.0254\n\nmeasured_num_points = 400\nmeasured_xs = np.linspace(-W/2, W/2, measured_num_points)\nmeasured_ys = np.linspace(-H/2, H/2, measured_num_points)\n\nmeasured_x_grid, measured_y_grid = np.meshgrid(measured_xs, measured_ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will define our test function. We are interested \nin generating a function that is representative \nof the full-field data we will use in calibrations \nand other MatCal studies. Generally, these data will be smooth, \nhave some lower frequency and higher frequency behavior \nand may have areas of localized high gradients and values. \nAs a result we choose, the following function which is an additive combination \nof three sinusoids and a linear function that is multiplied \nby a smooth function that approximates a dirac. This \nfunction is defined below: \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def analytical_function(X,Y):\n    small = H/20\n    func = (H/5 * np.sin(np.pi*Y/2/(H/2)) - W/50 * X/(W/2) \n           + H/40*np.sin(np.pi*Y/2/(H/20)) + W/100*np.sin(X/(W/20))) \\\n           * (1+small/(np.pi*(X**2+Y**2+small**2)))\n    return func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now evaluate the function on the measured grid and add\nnoise to it with a maximum amplitude of 2.5% of the maximum \nvalue of the function on the measured grid. We \nthen plot the function with the added noise to verify \nwe are producing the behavior we desire.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measured_func = analytical_function(measured_x_grid, measured_y_grid)\nrng = np.random.default_rng()\nnoise_amp = 0.025*np.max(measured_func)\nnoise = rng.random((measured_num_points, measured_num_points))*noise_amp-noise_amp/2\nmeasured_func += noise\n\nfrom matplotlib import cm\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nax.plot_surface(measured_x_grid, measured_y_grid,  measured_func, cmap=cm.coolwarm)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nax.set_zlabel(\"Z\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the measured data defined, we now create the simulation grid \nand the truth data for the simulation grid. As stated previously, the\nsimulation grid has 75% of the points of the measured grid and \nis defined on a 5% larger domain in both directions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim_num_points = 300\nsim_xs = np.linspace(-W/2*1.025, W/2*1.025, sim_num_points)\nsim_ys = np.linspace(-H/2*1.025, H/2*1.025, sim_num_points)\n\nsim_x_grid, sim_y_grid = np.meshgrid(sim_xs, sim_ys)\nsim_truth_func = analytical_function(sim_x_grid, sim_y_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the measured data and truth simulation data created, \nwe need to prepare the data to be used with the MatCal's\ninterface to the Compadre GMLS tool. To do so, we convert\nthe data to MatCal's field data class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measured_dict = {'x':measured_x_grid.reshape(measured_num_points**2), \n                 'y':measured_y_grid.reshape(measured_num_points**2), \n                 'val':measured_func.reshape(1, measured_num_points**2)}\nmeasured_data = convert_dictionary_to_field_data(measured_dict, coordinate_names=['x','y'])\n\nsim_truth_dict = {'x':sim_x_grid.reshape(sim_num_points**2), \n                  'y':sim_y_grid.reshape(sim_num_points**2), \n                  'val':sim_truth_func.reshape(1, sim_num_points**2)}\nsim_truth_data = convert_dictionary_to_field_data(sim_truth_dict, coordinate_names=['x','y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create a set of input parameters to \nevaluate using our test data sets. The two input \nparameters to the GMLS algorithm are the \nlocal polynomial order and the search radius multiplier. \nSince we are going to extrapolate, we know the polynomial order\nshould be relatively low. Also, since there is noise, we know \nwe will want the search radius to be large enough. For the Compadre\ntoolkit python interface, the search radius multiplier will multiply \nthe minimum search radius needed to fit the polynomial to the region \naround the current point of interest. For example, a polynomial of \norder 1 would require 3 points for our two dimensional domain. \nOur interface to the Compadre toolkit, will find the two nearest neighbors\nto the current point of interest. The default radius will be defined as \nthe largest distance between the current point of interest and \nthe two other points. The search radius multiplier then scales this radius \nto include more points in the local polynomial fit for the current point. \nThis is repeated for every point. \n\nTo study the influence of these parameters on our mapping tool,\nwe perform the mapping from our measured data \nto our simulation grid with polynomial orders of 1 to 3 with  \nsearch radius multipliers from 1.5 to 4. We than compare the \nmapped data to the known truth data on the simulation grid.\nWe start by specifying the input parameters of interest \nand importing the GMLS tool from MatCal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "polynomial_orders =[1,2,3]\nsearch_radius_mults = list(np.linspace(1.5,4, 11))\nsearch_radius_mults.append(5.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can loop over the parameters, map the \nmeasured function onto the simulation grid and\ncalculate the error fields and error measures \nfor the mapped field relative to the truth data\non the simulation grid. \nWe are interested in two error measures. The first \nerror measure we will investigate is the L2-norm \nof the error field normalized by the maximum of the \ntruth data on the simulation grid multiplied by 100. \nThis is a measure of the general quality of the fit\nfor each point on the grid it is calculated using\n\n\\begin{align}e_{norm} = 100\\frac{\\lVert f^h_s-f_s\\rVert_2}{m^2\\max\\left(f_s\\right)}\\end{align}\n\nwhere $f^h_s$ is the approximated function \nusing our GMLS mapping at the simulation grid points, \n$f_s$ is the known function evaluation at the simulation grid\npoints and $m$ is the number of points on one axis \nof the $m \\times m$ grid.\nThe second measure of error is the maximum error in the error \nfield for all points normalized by the maximum\nof the truth data function and multiplied by 100. This \ngives a maximum percent error for the mapped data field relative \nto the function maximum. It is calculated using\n\n\\begin{align}e_{max} = 100\\frac{\\lVert f^h_s-f_s\\rVert_{\\infty}}{\\max\\left(f_s\\right)}\\end{align}\n\nThe following code performs these calculations and stores the data \nin NumPy arrays so that they can be visualized next. It \nalso stores the data in a pickle file so that it can be \nread back later without recalculating it since the \ncomputational cost for these mappings is expensive\nfor the higher order polynomials and large\nsearch radius multipliers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "normalization_constant = np.max(sim_truth_func)\n\nerror_fields = []\nerror_norms = []\nerror_maxes = []\nfor poly_order in polynomial_orders:\n    error_fields_by_search_rad = []\n    error_norms_by_search_rad = []\n    error_maxes_by_search_rad = []\n    for search_rad_mult in search_radius_mults:\n        mapped_data = meshless_remapping(measured_data, [\"val\"], sim_truth_data.spatial_coords, \n                                         poly_order, search_rad_mult)\n        error_field = mapped_data[\"val\"]-sim_truth_data[\"val\"]\n        error_fields_by_search_rad.append(error_field)\n        error_norm = np.linalg.norm(error_field)/sim_num_points**2*100/normalization_constant\n        error_norms_by_search_rad.append(error_norm)\n        error_max = np.max(np.abs(error_field))/normalization_constant*100\n        error_maxes_by_search_rad.append(error_max)\n    error_fields.append(error_fields_by_search_rad)\n    error_norms.append(error_norms_by_search_rad)\n    error_maxes.append(error_maxes_by_search_rad)\n\nerror_fields = np.array(error_fields)\nerror_norms = np.array(error_norms)\nerror_maxes = np.array(error_maxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the error fields calculated, we can now create two heat maps \nshowing how our two error measures change as the polynomial order \nand search radius multiplier are varied. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from seaborn import heatmap\nimport matplotlib.colors as colors\n\nsearch_rad_mult_labels = [f\"{i:.2f}\" for i in search_radius_mults]\nplt.figure(\"$e_{{norm}}$\", figsize=(6,4), constrained_layout=True)\nheatmap(error_norms.T, annot=True, norm=colors.LogNorm(),\n         xticklabels=polynomial_orders, yticklabels=search_rad_mult_labels)\nplt.xlabel(\"polynomial order\")\nplt.ylabel(\"search radius multiplier\")\nplt.title(\"$e_{{norm}}$\")\n\nplt.figure(\"$e_{{max}}$\", figsize=(6,4), constrained_layout=True)\nheatmap(error_maxes.T, annot=True, norm=colors.LogNorm(),\n         xticklabels=polynomial_orders, yticklabels=search_rad_mult_labels)\nplt.xlabel(\"polynomial order\")\nplt.ylabel(\"search radius multiplier\")\nplt.title(\"$e_{{max}}$\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results are somewhat expected. From the $e_{max}$\nmeasure, we can see that linear polynomials do well at \nextrapolating. Since we applied a 2.5% noise centered \nat zero to the measured \nfield, the best we can expect with a perfect fit is a maximum \npercent error on the order of 1.25%. We obtain that maximum error with linear \npolynomials with a search radius multiplier that is relatively large\nnear 3.0. The other polynomial orders do not return that level \nof accuracy for any of the tested search radius multipliers and \nprovide large errors when the search radius is small. \nFrom the $e_{norm}$ measure, we see that overall\nthe GMLS approximation does fairly well at reproducing\nthe function $f$ once the search radius is large \nenough for all polynomials. Once again, the linear \npolynomial reaches the lowest values for the measure the quickest\nwhich is likely due to the extrapolation error. \n\nWe now visualize the produced error fields \nover the domain of interest for each \nset of mapping parameters used.  \n\n<div class=\"alert alert-info\"><h4>Note</h4><p>A power norm is used for the color bar of these plot.\n   The power norm is used to highlight the noise, but also \n   show the maximum error. A log scale could also be used, \n   but the level of noise was more clearly visualized with the\n   power norm and a gamma of 0.3.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_polys = len(polynomial_orders)\nnum_radiis = len(search_radius_mults)\nfig= plt.figure(f\"error fields for different mapping parameters\", \n                figsize=(5*num_polys, 5*num_radiis),\n                constrained_layout=True)\nmax_noise_error = noise_amp/2*100/normalization_constant\nfor row in range(num_polys):\n    for col in range(num_radiis):\n        ax = plt.subplot(num_radiis, num_polys,(row+1)+num_polys*col)\n        error_field = np.abs(error_fields[row, col].reshape(sim_num_points, \n                                                            sim_num_points)/\n                                                            normalization_constant*100)\n        levs = []\n        levs += list(np.linspace(0, max_noise_error, 6))\n        max_err = np.max(error_field)\n        if max_err > max_noise_error*3:\n            levs += [max_err/2, max_err]\n        elif max_err > max_noise_error:\n            levs += [max_err]\n        cs = ax.contourf(sim_x_grid, sim_y_grid, error_field, levs, \n                         norm = colors.PowerNorm(gamma=0.3), cmap='magma')\n        plt.xlabel(\"X\")\n        plt.ylabel(\"Y\")\n        plt.title(f\"polynomial order {row+1}\\nsearch \" + \n                  f\"radius multiplier {search_radius_mults[col]:1.2f}\")\n        plt.colorbar(cs, ax=ax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From these plots, four conclusions are clear. \n\n#.  error is highest in the extrapolation regions \n    on the domain boundaries and the extrapolation error \n    increases with polynomial order. As observed earlier, \n    this is an expected result. \n#.  Second, the noise level\n    seems higher in the order one polynomial. This result\n    is expected because the high order polynomials require \n    more neighbors and will result in a much larger window \n    for a given search radius multiplier for this evenly spaced \n    set of grids. By including more points, in the local least\n    squares fit about a point more filtering of the noise is expected.\n#.  Third, for all polynomials orders, as the the search\n    radius multiplier is increased the amount of \n    filtering is also increased. \n#.  Fourth, the higher order polynomials perform better \n    at the higher search radius multipliers at reproducing\n    the smooth function of interest. For search \n    radius multipliers greater than 3, the linear polynomial\n    option produces noticeable error around the center \n    where the smooth dirac function has a high amplitude.\n\nBased on these findings, the default settings for the MatCal \nmapping settings are a polynomial order of 1 and a search radius \nmultiplier of 2.75 with the goal of balancing speed and accuracy\nespecially when extrapolation may occur. \n\nThese settings are best suited for mapping problems with the following \ncharacteristics:\n\n#. The underlying function being studied is relatively smooth \n   when compared to the discretization point cloud spacing. In \n   other words, the point cloud spacing should be significantly\n   smaller than the size of the features of interest for the function\n   that they hold data for.\n#. The data being mapped has to extrapolate a small amount away from the \n   source data. \n#. The noise in the data is small relative to the magnitude of the field \n   of interest and only a small amount of filtering is desired.\n\nWhen not extrapolating and some level of filtering is desired, \nthe polynomial order can be increased. If extrapolating and \nmore filtering is desired, a polynomial order of one is highly recommended, \nbut the search radius can be increase significantly. \n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Increasing either mapping parameter\n   will noticeable increase run time and memory consumption.</p></div>\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}