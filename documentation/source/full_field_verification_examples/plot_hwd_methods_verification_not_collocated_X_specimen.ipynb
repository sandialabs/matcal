{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Polynomial HWD Verification - X Specimen Data\nIn this example, we test and verify the \nPolynomial HWD algorithm using experimental \ndata from :cite:p:`VFM_LDRD_Jones`. \nWe evaluate the method's sensitivity to \nwhich point cloud is used to generate \nthe HWD basis functions. As we will show,\nthe choice is important and affects the \nvalidity of the HWD weights and the \nquality of the reconstructed fields. \n\nThis test is performed \non the experimental data\nfor one of the X specimens (XR4) and \nthe same data that has been mapped \nto a simulation mesh surface using \nMatCal's :func:`~matcal.full_field.field_mappers.meshless_remapping`\nfunction.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We are operating on actual data so an analytical solution is unavailable.\n    However, our previous verification example \n    `Full-field Interpolation Verification`\n    indicated the error in mapped data should \n    be on the order of measured noise or less. Since the noise \n    in this data is low relative to the field of interest, \n    most of the error that will be shown is due to the HWD field\n    reconstruction.</p></div>\n\nWe will compare these \ndata twice, once with the experimental \ndata as source for the basis functions and \nonce with the mapped data as the source for these basis functions.\nThese sets of basis functions will be referred to as \nthe experimental basis and mapped basis, respectively.\nIn these comparisons, we evaluate the convergence of the \nHWD weights for the two fields and the quality \nof the reconstructed fields when using the different\nbases.  \n\nTo begin we import the libraries and tools we will \nbe using to perform this study.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n\nplt.rc('text', usetex=True)\nplt.rc('font', family='serif')\nplt.rcParams.update({'font.size': 12})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we import the experimental data we \nwill use for the study. We have already \nprocessed the data and extracted the field \ndata at peak load where the displacement field \nis acceptably resolved with the digital image correlation (DIC)\nsoftware and the geometry is highly deformed. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fields = ['V']\nexp_file_data = FileData(\"x_specimen_XR4_peak_load_data.csv\")\nexp_data = convert_dictionary_to_field_data({\"time\":[0], \n    fields[-1]:exp_file_data[fields[-1]].reshape(1, len(exp_file_data[fields[-1]]))})\nspatial_coords = np.array([exp_file_data[\"X0\"], exp_file_data[\"Y0\"]]).T\nexp_data.set_spatial_coords(spatial_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we import the node locations from \na mesh of the geometry. The mesh \nhas a coarser resolution of the geometry \nthan the experimental data\nand covers nearly the same area. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_sim_node_locations = FileData(\"sim_X_specimen_locs.csv\")\nX_sim_node_locations = np.array([X_sim_node_locations[\"X\"], X_sim_node_locations[\"Y\"]]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MatCal :func:`~matcal.full_field.field_mappers.meshless_remapping`\n function is used to perform the interpolation from the \n experimental data points to the mesh node locations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gmls_mapped_data = meshless_remapping(exp_data, [fields[-1]],\n                                      X_sim_node_locations,\n                                      polynomial_order=1,\n                                      search_radius_multiplier=2.75)\ngmls_mapped_data.set_spatial_coords(X_sim_node_locations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the interpolation,\nwe plot the vertical displacement field V \nto visualize the data and  \nthe point clouds were data exists. \nThe absolute value of the data \nis plotted on a log scale. \nThis is shown to make any potential \nnoise visible.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_field(data, field, ax):\n    c = ax.scatter(data.spatial_coords[:,0], data.spatial_coords[:,1], \n                   c=np.abs(data[field]), marker='.', s=1, \n                   norm=colors.LogNorm())\n    ax.set_xlim([-0.039, 0.039])\n    ax.set_ylim([-0.065, 0.065])\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    return c\n\nfig, axes = plt.subplots(1,2, constrained_layout=True)\nc =plot_field(exp_data, fields[-1], axes[0])\nplot_field(gmls_mapped_data, fields[-1], axes[1])\nfig.colorbar(c, ax=axes[1])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is no clearly discernable noise in the \ndata, but there is notable data loss near the center where \nthe specimen has begun to plastically localize.\nSuch data loss is inevitable and the methods\nshould be usable even with missing points.\nAlso, the data have been plotted in figures \nwith the same X and Y axes limits. \nThis was done to more clearly show \nthat the simulation mesh covers more \nsurface area than the data generated\nusing the DIC software. This \nis an expected result due to the limitations\nof most common DIC software, and is \nnot an accurate representation of the \ntotal surface geometry. The simulation mesh\nwas built to accurately cover the total part \nsurface assuming the part was made \nclose to nominal dimensions and within \ntolerance.\nAs we will see, the reduced area for the DIC \nmeasurement fields is the primary \ncause of the errors and issues\nthat this example will highlight. \n\nNow we create a set of input parameters to \nevaluate using our data sets. The two input \nparameters to the HWD algorithm are the \npolynomial order of the pattern functions \nand the depth of subdivision tiers in the splitting tree.\nTo study the influence of these parameters on our mapping tool,\nwe perform the mapping with polynomial orders 1 to 4 and depths of 4 to 8.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "polynomial_orders = np.array([1, 2, 3, 4, 6, 8], dtype=int)\ncut_depths = np.array( [4, 6, 8], dtype=int)\nnum_polys = len(polynomial_orders)\nnum_depths = len(cut_depths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MatCal tool that will be evaluated \nis the QoI extractor that \nperforms the HWD operations  \nfor the :class:`~matcal.full_field.objective.PolynomialHWDObjective` \nobjective when not used with point collocation.\nThe :class:`~matcal.full_field.qoi_extractor.HWDPolynomialSimulationSurfaceExtractor`\nis used to build the HWD basis and calculate the HWD weights for  \nboth data sets and both sets of basis functions.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>The QoI extractors are not meant for direct use by users. The interfaces will likely \n  change in future releases. Also, the names are specific for their use underneath \n  user facing classes and may not be indicative of how they are used here.</p></div>\n\nWe put the initialization of the HWD QoI extractor\nand calculation of our HWD weights and \nreconstruction into a function so \nthat we can call it with the different\nHWD input parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal.full_field.qoi_extractor import HWDPolynomialSimulationSurfaceExtractor\n\ndef get_HWD_results(poly_order, cut_depth, basis_data, comparison_data):\n    print(f\"Running Depth {cut_depth}, Order {poly_order}\")\n    hwd_extractor = HWDPolynomialSimulationSurfaceExtractor(basis_data.skeleton, \n                                                            int(cut_depth), \n                                                            int(poly_order), \"time\")\n\n    comparison_weights = hwd_extractor.calculate(comparison_data, \n                                                 comparison_data, ['V'])            \n    basis_weights = hwd_extractor.calculate(basis_data, comparison_data, ['V'])\n\n    reconstructed_field = hwd_extractor._hwd._Q.dot(comparison_weights['V'])\n    reconstructed_error_field = (reconstructed_field - basis_data['V'])\n    print(f\"Depth {cut_depth}, Order {poly_order} finished.\")\n    return basis_weights['V'], comparison_weights['V'], reconstructed_error_field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a function that loops over the\nHWD method input parameters, generates \nthe HWD basis,  stores the \nHWD weight values and stores the reconstructed \nerror fields for our comparison.\nSince we will perform these operations twice using the different \nbasis functions sets, putting the \ncalculations in a function simplifies the process.\nThe following code performs these calculations and stores the data \nin NumPy arrays so that they can be visualized next. It \nalso stores the data in a pickle file so that they can be \nloaded later without recalculating since the \ncomputational cost for these mappings can be expensive.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate_errors(basis_data, comparison_data):\n    from concurrent.futures import ProcessPoolExecutor\n    futures = {}\n    with ProcessPoolExecutor(max_workers = num_depths*num_polys) as executor:    \n        for p_index,poly_order in enumerate(polynomial_orders):\n            futures[poly_order] = {}\n            for d_index, depth in enumerate(cut_depths):\n                futures[poly_order][depth] = get_HWD_results(poly_order, depth, \n                                                             basis_data, \n                                                             comparison_data)  \n    \n#                futures[poly_order][depth] = executor.submit(get_HWD_results, \n#                                                                 poly_order, depth, \n#                                                                 basis_data, \n#                                                                 comparison_data)  \n#    \n    reconstructed_error_fields = np.zeros((num_polys, num_depths, 1, \n                                           basis_data.spatial_coords.shape[0]))\n    all_comparison_weights = []\n    all_basis_weights = []\n    for p_index,poly_order in enumerate(polynomial_orders):\n        comparison_weights_fields_by_depth = []\n        basis_weights_fields_by_depth = []\n        for d_index, depth in enumerate(cut_depths):\n#            results = futures[poly_order][depth].result()\n            results = futures[poly_order][depth]\n            basis_weights_fields_by_depth.append(results[0])\n            comparison_weights_fields_by_depth.append(results[1])\n            reconstructed_error_fields[p_index,d_index]  = results[2]          \n        all_comparison_weights.append(comparison_weights_fields_by_depth)\n        all_basis_weights.append(basis_weights_fields_by_depth)\n\n    results_dict = {\"comparison weights\":all_comparison_weights, \n                    \"basis weights\":all_basis_weights, \n                    \"error fields\":reconstructed_error_fields}\n    return results_dict\n\nexp_basis_results = evaluate_errors(exp_data, gmls_mapped_data)\nmapped_basis_results = evaluate_errors(gmls_mapped_data, exp_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we will look at how the HWD weights change \nwhen using the different basis functions.\nWe are interested in two measures for the weights. The first \nerror measure is the L2-norm \nof the normalized HWD weight error field multiplied by 100. \nThis is a general measure of how well the HWD weights are \nreproduced for the nearly-equivalent field when \ncalculated from different discretizations.\nThis error measure is calculated using\n\n\\begin{align}e_{norm} = \\frac{100}{\\sqrt{m}} \\frac{\\lVert w_{comp}-w_{basis} \\rVert_2}{\\max\\left(w_{basis}\\right)}\\end{align}\n\nwhere $w_{comp}$ are the weights generated from the data\nnot used to generate the basis functions,  \n$w_{basis}$ are the weights generated from the data\nused to generate the basis functions and $m$\nis the number of the weights generated.\nThe second measure of error is the maximum error in the \ncomparison field \nweights normalized by the maximum\nweight from the weights calculated for the \ndata used to generate the basis functions.\nThis error is also multiplied by 100 to give \na maximum percent error for weights relative \nto the weights maximum. It is calculated using\n\n\\begin{align}e_{max} = 100\\frac{\\lVert w_{comp}-w_{basis}\\rVert_{\\infty}}{\\max\\left(w_{basis}\\right)}\\end{align}\n\nWe create a function that calculates these \nerror metrics given the \nweight errors for each input parameter. \nWe then use that function to calculate \nthe error metrics for our two \ndifferent comparisons.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def calculate_weights_error_metrics(comparison_weights, basis_weights):\n    weight_error_norms = np.zeros((num_polys, num_depths))\n    weight_error_maxes = np.zeros((num_polys, num_depths))\n    for p_index in range(num_polys):\n        for d_index in range(num_depths):\n            weight_error_vec = (comparison_weights[p_index][d_index] - \n                                basis_weights[p_index][d_index])\n            length_normalization = len(weight_error_vec)\n            val_normalization = np.max(basis_weights[p_index][d_index])\n            weight_error_norms[p_index, d_index] = 100*np.linalg.norm(weight_error_vec)/ \\\n                np.sqrt(length_normalization)/val_normalization\n            weight_error_maxes[p_index, d_index] = 100*np.max(np.abs(weight_error_vec))/ \\\n                val_normalization\n    return weight_error_norms, weight_error_maxes\n\nresults = calculate_weights_error_metrics(exp_basis_results[\"comparison weights\"],\n                                          exp_basis_results[\"basis weights\"])\nexp_basis_weight_norms, exp_basis_weight_maxes = results\nresults = calculate_weights_error_metrics(mapped_basis_results[\"comparison weights\"], \n                                          mapped_basis_results[\"basis weights\"])\nsim_basis_weight_norms, sim_basis_weight_maxes = results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the error fields calculated, we can now create two heat maps \nshowing how our two error measures change as the polynomial order \nand cut depths are varied. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from seaborn import heatmap\n\ndef plot_heatmap(data, title):\n    heatmap(data.T, annot=True, norm=colors.LogNorm(),\n            xticklabels=polynomial_orders, yticklabels=cut_depths)\n    plt.title(title)\n    plt.xlabel(\"polynomial order\")\n    plt.ylabel(\"max depth\")\n\nfig = plt.figure(figsize=(10,10), constrained_layout=True)\nax = plt.subplot(2,2,1)\nplot_heatmap(exp_basis_weight_norms, \"Exp Basis $e_{{norm}}$\")\nax = plt.subplot(2,2,2)\nplot_heatmap(exp_basis_weight_maxes, \"Exp Basis $e_{{max}}$\")\nax = plt.subplot(2,2,3)\nplot_heatmap(sim_basis_weight_norms, \"Mapped Basis $e_{{norm}}$\")\nax = plt.subplot(2,2,4)\nplot_heatmap(sim_basis_weight_maxes, \"Mapped Basis $e_{{max}}$\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From these heat maps, it is clear that \nthe weights match better for those \ngenerated using the experimental basis. \nAs polynomial order and depth increases, both error \nmetrics tend to decrease. While for the \nmapped basis, we see\nthat these errors begin to get larger for\nhigher polynomial orders and cut depths.\nTo investigate this further, we plot the weights  \ngenerated using the different basis function sets.\nWe plot these for the combination of inputs \nthat produced the best agreement for \nthe weights for the experimental basis and the\nworst agreement for the weights the mapped basis.\nThe inputs used to generate these weights \nare polynomial order six and cut depth eight for both \nsets of basis functions.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def setup_plot():\n    plt.xlim([0,2000])\n    plt.ylim([10e-6, 10e0])\n    plt.ylabel(\"Normalized Weight\")\n    plt.legend()\nfig = plt.figure(constrained_layout=True)\nplt.subplot(2,1,1)\nbasis_weights = exp_basis_results[\"basis weights\"][-1][-1]\ncomp_weights = exp_basis_results[\"comparison weights\"][-1][-1]\nplt.semilogy(basis_weights/np.max(basis_weights), label=\"exp weights\")\nplt.semilogy(comp_weights/np.max(basis_weights), label=\"mapped weights\", linestyle=\"--\")\nsetup_plot()\nplt.title(\"Exp Basis\")\n\nfig = plt.figure(constrained_layout=True)\nplt.subplot(2,1,2)\nbasis_weights = mapped_basis_results[\"basis weights\"][-1][-1]\ncomp_weights = mapped_basis_results[\"comparison weights\"][-1][-1]\nplt.semilogy(comp_weights/np.max(basis_weights), label=\"exp weights\")\nplt.semilogy(basis_weights/np.max(basis_weights), label=\"mapped weights\", linestyle=\"--\")\nsetup_plot()\nplt.title(\"Mapped Basis\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From these plots it is apparent \nthat the experimental basis \nprovides nearly equivalent weights \nfor the two different discretizations \nand data sources. The mapped basis\nshows noticeable differences for \nmost if not all modes\nwhen HWD is applied \nto the different data sets. To better\nunderstand why, we will plot the error fields\nfor both the mapped and experimental bases. \n\nThese plots also indicate the \nlevel of data compression \nprovided for the polynomial \norder 6 and depth cut 8 example.\nThe full point cloud is \nrepresented by the ~2500 basis\nfunction weights and the transformation \nmatrix which is a matrix with a size of \nthat is governed by the number of basis functions \nand the number of polynomial coefficients. If the \nsame basis functions are used for each data set, \neach field in the data\nand each time step, only the mode weights need to be \nstored for each time step after the \ninitial step. This can result in significant data compression\nwith minimal loss.\n\nTo understand \nthe effect of the weight errors, \nwe now visualize the error fields \nover the domain of interest for each \nset of basis functions. We do so for a \nreduced set \nof input parameters to the HWD method.  \n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The color bar scale is changing for each of these plots.\n   When looking at the data, be cognizant of the changes \n   to the color bar maximums and minimums.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_value = np.max(np.abs(exp_data['V']))\ndef plot_error_fields(error_fields, fig_name, coords, vmax):\n    poly_start_index = 2\n    depth_start_index = 1\n    viewed_polys = polynomial_orders[poly_start_index:]\n    viewed_depths =  cut_depths[depth_start_index:]\n    fig, ax_set = plt.subplots(len(viewed_polys), len(viewed_depths), \n                               figsize=(5*len(viewed_depths), 5*len(viewed_polys)))\n    for row, po in enumerate(viewed_polys):\n        ax_set[row,0].set_ylabel(f\"Order: {po}\")\n        for col, depth in enumerate(viewed_depths):                \n            ax = ax_set[row, col]\n            if row == 0:\n                ax.set_title(f\"Depth: {depth}\")\n            error_field = error_fields[row+poly_start_index, \n                                                    col+depth_start_index]\n            error_field = np.abs(error_field/max_value*100)\n            cs = ax.scatter(coords[:,0], coords[:, 1], c=error_field.flatten(), \n                            norm = colors.LogNorm(vmin=1e-3, vmax=vmax), \n                            cmap='magma', marker='.', s=.9)\n            ax.set_yticklabels([])\n            ax.set_xticklabels([])\n            ax.set_xticks([])\n            ax.set_yticks([])\n    fig.colorbar(cs, ax=ax_set.ravel())\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we observe the reconstructed\nfield errors generated \nusing the mapped basis and associated weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_error_fields(mapped_basis_results[\"error fields\"], \"Sim Basis Error Fields\", \n                  gmls_mapped_data.spatial_coords, 1e2)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Two trends are clear.\n\n#. As the depth increases, the largest errors\n   isolate \n   near edges. \n#. As the polynomial order increases,\n   this error increases significantly. \n\nFor the mapped basis, the HWD method \nis extrapolating near the edges \nwhere the experimental data has\nfew points to support the polynomials.\nThe inconsistent weight errors \nshown previously are, therefore, a \nresult of extrapolation in these areas \nwhere the HWD mapped basis functions \nare not suitable for extrapolation given \nthe limited points available from the\nexperimental data. Although the linear \norder polynomials perform much better, \nthey do not converge very quickly with \nincreasing cut depth. This could be partially\nremedied by different domain decompositions \nwhich are an area of future research.\n\nWe now look at the same error fields \nfor the experimental basis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_error_fields(exp_basis_results[\"error fields\"], \"Exp Basis Error Fields\", \n                  exp_data.spatial_coords, 5)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These results show that when there is \nenough support for the basis functions \nthe field reconstruction performs well. \nHowever, when the smaller domain is used\nto generate the basis functions, \nextrapolation may result in large \nerrors as the method extrapolates to points\nin the larger domain without sufficient \nsupport. \n\nSince domains of different sizes are common \nfor field data comparisons, MatCal uses our \n`Full-field Interpolation and Extrapolation`\nmethods to move data to a common domain before using \nHWD methods to operate on the data. By default, we map \nto the simulation domain because this usually\nresults in less memory use. With current capabilities\nthese default settings should result in the most\nrobust and efficient usage of the HWD tools. \n\nFuture work will involve overcoming some of these \nlimitations to improve efficiency. Some potential \nsolutions include:\n\n#. Domain identification and matching. \n   MatCal will identify the portions of the \n   domain that do not overlap and remove them \n   from the comparison.\n#. Improved domain decomposition. MatCal will \n   create different subsections based on  \n   the fields being analyzed, the specifics of the\n   geometry or both.\n#. Mapping the given geometry onto a simpler \n   geometry. If geometries can be mapped \n   to a unit square with a transformation, \n   this would make domain decomposition trivial.\n\nAlthough the potential research methods could improve\nefficiency to some extent, the current implementation \nwith GMLS mapping to a common mesh is robust and \nprovides significant memory reduction compared to\ncomparing full-field data through interpolation \nalone.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}