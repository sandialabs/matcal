{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Polynomial HWD Verification with Colocated Points\nIn this example, we use an analytical function \nto test and verify our use and implementation of the \nPolynomial HWD algorithm where the points are\ncolocated. The colocation is done using \nour PyCompadre's GMLS algorithm :cite:p:`compadre_toolkit`.\nWe use the algorithm to map the experimental \ndata onto the simulation points. Once this is done, \nthe HWD method can be used to compare the data.  \nWe designed the verification problem to be \nrepresentative of intended application and use \nfull-field data that captures much of the \nexpected behavior seen in real data sets.\nThis test will compare the significant weights of the \nfull-field data in their compressed latent space. Since \nthe HWD modes are guaranteed to be the same, comparing the \nweights will be a valid comparison of the field as \nlong as the field accurately reproduced with\nsufficient modes and as long as the \nGMLS interpolation doesn't adversely affect\nthe field.\n\nOnce again, we will generate two instances\nof our full-field data with noise and strive to have their difference be as \nsmall as possible. The same procedure is used here a was used\nin `Polynomial HWD Verification - Analytical Function`.\n\nTo begin we import the libraries and tools we will be using to perform this study.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal import *\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rc('text', usetex=True)\nplt.rc('font', family='serif')\nplt.rcParams.update({'font.size': 12})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we defined the domain for our measured grid using NumPy tools.\nThe domain is about 15 mm high (6 inches) and 7.6 mm wide (3 inches). \nThe measured grid has 300 points in each dimension (x, y).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "H = 6*0.0254\nW = 3*0.0254\n\nmeasured_num_points = 300\nmeasured_xs = np.linspace(-W/2, W/2, measured_num_points)\nmeasured_ys = np.linspace(-H/2, H/2, measured_num_points)\n\nmeasured_x_grid, measured_y_grid = np.meshgrid(measured_xs, measured_ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will define our test function. We are interested \nin generating a function that is representative \nof the full-field data we will use in calibrations \nand other MatCal studies. Generally, these data will be smooth, \nhave some lower frequency and higher frequency behavior \nand may have areas of localized high gradients and values. \nAs a result we choose, the following function which is an additive combination \nof three sinusoids and a linear function that is multiplied \nby a smooth function that approximates a dirac. This \nfunction is defined below: \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def analytical_function(X,Y):\n    small = H/10\n    func = (H/5 * np.sin(np.pi*Y/2/(H/2)) - W/50 * X/(W/2) \n           + H/40*np.sin(np.pi*Y/2/(H/20)) + W/100*np.sin(X/(W/20))) \\\n           * (1+small/(np.pi*(X**2+Y**2+small**2)))\n    return func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now evaluate the function on the measured grid and add\nnoise to it with a maximum amplitude of 2.5% of the maximum \nvalue of the function on the measured grid. We \nthen plot the function with the added noise to verify \nwe are producing the behavior we desire.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measured_func = analytical_function(measured_x_grid, measured_y_grid)\nrng = np.random.default_rng() \nnoise_amp = 0.025*np.max(measured_func)\nnoise_multiplier = rng.random((measured_num_points, measured_num_points)) - .5 \nnoise = noise_multiplier*noise_amp\nmeasured_func += noise\n\nfrom matplotlib import cm\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nax.plot_surface(measured_x_grid, measured_y_grid,  measured_func,\n                cmap=cm.coolwarm)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nax.set_zlabel(\"Z\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the measured data defined, we now create the simulation point cloud\nand the truth data for the simulation point cloud. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim_num_points = measured_num_points\nsim_xs = np.random.uniform(-W/2, W/2, sim_num_points**2)\nsim_ys = np.random.uniform(-H/2, H/2, sim_num_points**2)\n\nsim_truth_func = analytical_function(sim_xs, sim_ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the measured data and truth simulation data created, \nwe need to prepare the data to be used with the MatCal's\ninterface to the HWD tool. To do so, we create\na :class:`~matcal.full_field.data.FieldData` object for\nboth data sets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measured_dict = {'x':measured_x_grid.reshape(measured_num_points**2), \n                 'y':measured_y_grid.reshape(measured_num_points**2), \n                 'val':measured_func.reshape(1, measured_num_points**2),\n                 'time':np.array([0])}\nmeasured_data = convert_dictionary_to_field_data(measured_dict, \n                                                 coordinate_names=['x','y'])\n\nsim_truth_dict = {'x':sim_xs, \n                  'y':sim_ys, \n                  'val':sim_truth_func.reshape(1, sim_num_points**2), \n                  'time':np.array([0])}\nsim_truth_data = convert_dictionary_to_field_data(sim_truth_dict, \n                                                  coordinate_names=['x','y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create a set of input parameters to \nevaluate using our test data sets. The two input \nparameters to the HWD algorithm are the \npolynomial order of the pattern functions and the depth of subdivision tiers in the splitting tree.\n\nTo study the influence of these parameters on our mapping tool,\nwe perform the mapping with polynomial orders of increasing \npolynomial orders from 1 to 8 and depths of 4 to 10.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "polynomial_orders = np.array([8, 6, 4, 3, 2, 1], dtype=int) #1,2,3,4,6,8\ncut_depths = np.array([10, 8, 6, 4, 2], dtype=int)#4,6,8,10\nnum_polys = len(polynomial_orders)\nnum_depths = len(cut_depths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then setup a function to compare the HWD weights produced \nfrom the the noisy\nexperimental data to the HWD weights produced from \nthe known truth data on the simulation grid.\nWe will be using the \n:class:`~matcal.full_field.qoi_extractor.HWDPolynomialSimulationSurfaceExtractor`\nand :class:`~matcal.full_field.qoi_extractor.HWDColocatingExperimentSurfaceExtractor`\nclasses\nto perform the HWD operations on our data. \n\n .. warning::\n  The QoI extractors are not meant for direct use by users. The interfaces will likely \n  change in future releases. Also, the names are specific for their use underneath \n  user facing classes and may not be indicative of how they are used here.\n\nThis function requires the HWD tool input parameters of \npolynomial order and cut depth. It also requires that \ntwo evaluations of the function so that it \ncan use the QoI extractors to calculate the fields and \nHWD weights. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal.full_field.qoi_extractor import HWDColocatingExperimentSurfaceExtractor, \\\n     HWDPolynomialSimulationSurfaceExtractor\n\ndef get_HWD_results(poly_order, cut_depth, sim_truth_data, measured_data):\n    print(f\"Running Depth {cut_depth}, Order {poly_order}\")\n    sim_extractor = HWDPolynomialSimulationSurfaceExtractor(sim_truth_data.skeleton, \n                                                            int(cut_depth), int(poly_order), \n                                                            \"time\")\n    measured_coords = measured_data.skeleton.spatial_coords[:,:2]\n    sim_coords = sim_truth_data.skeleton.spatial_coords[:,:2]\n    exp_extractor = HWDColocatingExperimentSurfaceExtractor(sim_extractor, \n                                                            measured_coords, \n                                                            sim_coords)\n\n    measured_weights = exp_extractor.calculate(measured_data, measured_data, ['val'])            \n    truth_weights = sim_extractor.calculate(sim_truth_data, measured_data, ['val'], False)\n\n    reconstructed_sim = sim_extractor._hwd._Q.dot(measured_weights['val'])\n    reconstructed_error_field = (reconstructed_sim - sim_truth_data['val'])\n    print(f\"Depth {cut_depth}, Order {poly_order} finished.\")\n\n    return truth_weights['val'], measured_weights['val'], reconstructed_error_field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can loop over the parameters, generate \nthe HWD basis and store the values \nthat we will be plotting next. These evaluations\nare computationally expensive. As a result, we \nuse Python's ProcessPoolExecutor to \nrun the function in parallel for each \nset of HWD input parameters to speed the calculations.\nWe also store the results in a pickle file so\nthat they are not needlessly recalculated.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_sim_value = np.max(np.abs(sim_truth_data['val']))\nfrom concurrent.futures import ProcessPoolExecutor\nfutures = {}\nwith ProcessPoolExecutor(max_workers = int(num_depths*num_polys/3)) as executor:    \n    for p_index, poly_order in enumerate(polynomial_orders):\n        futures[poly_order] = {}\n        for d_index, depth in enumerate(cut_depths):\n            futures[poly_order][depth] = get_HWD_results(poly_order, depth, \n                                                         sim_truth_data, measured_data)           \n#            futures[poly_order][depth] = executor.submit(get_HWD_results, \n#                                                         poly_order, depth, \n#                                                         sim_truth_data, measured_data)           \n\nreconstructed_error_fields = np.zeros((num_polys, num_depths, 1, \n                                       sim_truth_data.spatial_coords.shape[0]))\nall_measured_weights = {}\nall_truth_weights = {}\nfor p_index, poly_order in enumerate(polynomial_orders):\n    all_measured_weights[poly_order] = {}\n    all_truth_weights[poly_order] = {}\n    for d_index, depth in enumerate(cut_depths):\n#        results = futures[poly_order][depth].result()\n        results = futures[poly_order][depth]\n        all_truth_weights[poly_order][depth] = results[0]\n        all_measured_weights[poly_order][depth] = results[1]\n        reconstructed_error_fields[p_index,d_index]  = results[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are interested in two error measures. The first \nerror measure we will investigate is the L2-norm \nof the error field normalized by the maximum of the \ntruth data by 100. \nThis is a measure of the general quality of the fit\nfor each point being evaluated and is calculated using\n\n\\begin{align}e_{norm} = \\frac{100}{\\sqrt{m}}  \\frac{\\lVert v_{exp}-v_{sim} \\rVert_2}{\\max\\left(v_{sim}\\right)}\\end{align}\n\nwhere $v_{exp}$ are the experiment values, \n$v_{sim}$ are the known values and \n$m$ is the number of values being compared.\nThe second measure of error is the maximum error  \nbetween the values from the different \nsources divided by the maximum\nof the truth data and multiplied by 100. This \ngives a maximum percent error for the data \nrelative to the truth data. \nIt is calculated using\n\n\\begin{align}e_{max} = 100\\frac{\\lVert v_{exp}-v_{sim}\\rVert_{\\infty}}{\\max\\left(v_{sim}\\right)}\\end{align}\n\nThese functions are valid for both the HWD weights and function evaluations \ncalculated for\neach discretization.\n\nThe following code performs these calculations and stores the data \nin NumPy arrays so that they can be visualized. It \nalso stores the data in a pickle file so that it can be \nread back later without recalculating since the \ncomputational cost for these calculations can be expensive.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def calculate_error_metrics(measured_fields, truth_fields=None):\n    error_norms = np.zeros((num_polys, num_depths))\n    error_maxes = np.zeros((num_polys, num_depths))\n    for p_index, poly_order in enumerate(polynomial_orders):\n        for d_index, depth in enumerate(cut_depths):\n            if truth_fields:\n                error_vec = (measured_fields[poly_order][depth] - truth_fields[poly_order][depth])\n                val_normalization = np.max(truth_fields[poly_order][depth])\n            else:\n                error_vec = measured_fields[p_index,d_index].flatten()\n                val_normalization = max_sim_value\n            length_normalization = len(error_vec)\n            error_norms[p_index, d_index] = 100 * np.linalg.norm(error_vec) / np.sqrt(length_normalization) / val_normalization\n            error_maxes[p_index, d_index] = 100 * np.max(np.abs(error_vec)) / val_normalization\n    return error_norms, error_maxes\n\nweight_error_norms, weight_error_maxes = calculate_error_metrics(all_measured_weights, all_truth_weights)\nfield_error_norms, field_error_maxes = calculate_error_metrics(reconstructed_error_fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the error fields calculated, we can now create four heat maps \nshowing how our four error measures change as the polynomial order \nand cut depth are varied. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from seaborn import heatmap\nimport matplotlib.colors as colors\n\ndef plot_heatmap(data, title):\n    heatmap(data.T,  annot=True, \n            norm=colors.LogNorm(vmax=1e3),\n            xticklabels=polynomial_orders,\n            yticklabels=cut_depths)\n    plt.title(title)\n    plt.xlabel(\"polynomial order\")\n    plt.ylabel(\"max depth\")\n\nfig = plt.figure(figsize=(10,10), constrained_layout=True)\nax = plt.subplot(2,2,1)\nplot_heatmap(weight_error_norms, \"Weights $e_{{norm}}$\")\nax = plt.subplot(2,2,2)\nplot_heatmap(weight_error_maxes, \"Weights $e_{{max}}$\")\nax = plt.subplot(2,2,3)\nplot_heatmap(field_error_norms, \"Field $e_{{norm}}$\")\nax = plt.subplot(2,2,4)\nplot_heatmap(field_error_maxes, \"Field $e_{{max}}$\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this test, the error measures using the \nweights \nare relatively flat for all inputs.  Since these fields \nare measured at the same points after interpolation, \nthis is an expected result.  We have shown the \nGMLS interpolation works well as implemented\nin `Full-field Interpolation Verification`\nSince the data is well interpolated, \nthis is highlighting that we are essentially performing the \nsame transform twice. Therefore, the error in the weights \nare due partially to noise and partially to interpolation error\nfrom the GMLS interpolation. It should be noted that even \nif the weights match for any HWD inputs, there is no guarantee \nthat the fields will match if the HWD transform is non-unique.\nFor low polynomial order and low cut depth HWD bases, the odds \nof a non-unqiue transform is higher. \n\nAs a result, we look \nto the error fields to determine what polynomial order\nand cut depth is required to adequately reconstruct the field. \nWe visualize the produced error fields \nover the domain of interest for the polynomial orders of three to six \nand cut depths of six to ten.  \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "poly_start_index = 1\ndepth_start_index = 0\nviewed_polys = polynomial_orders[poly_start_index:-2]\nviewed_depths =  cut_depths[depth_start_index:-2]\nfig, ax_set = plt.subplots(len(viewed_polys), len(viewed_depths),\n                            figsize=(5*len(viewed_depths), 5*len(viewed_polys)))\nfor row, po in enumerate(viewed_polys):\n    ax_set[row,0].set_ylabel(f\"Order: {po}\")\n    for col, depth in enumerate(viewed_depths):                \n        ax = ax_set[row, col]\n        if row == 0:\n            ax.set_title(f\"Depth: {depth}\")\n        error_field = reconstructed_error_fields[row+poly_start_index, \n                                                 col+depth_start_index]\n        error_field = np.abs(error_field/max_sim_value*100)\n        cs = ax.scatter(sim_xs, sim_ys, c=error_field.flatten(), \n                        norm = colors.LogNorm(vmin=1e-2, vmax=1e1), \n                        cmap='magma', marker='.', s=.9)\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        ax.set_xticks([])\n        ax.set_yticks([])\nfig.colorbar(cs, ax=ax_set.ravel())\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From these plots, the following conclusions can be made: \n\n#.  Recreation error is highest in the central peak region. \n    Increasing polynomial order and depth better characterized \n    the local behavior at this location. Increasing the depth \n    of HWD allowed for more support of the central region. \n    Increasing the polynomial order added additional flexibility \n    to the wave forms allowing for a more accurate reconstruction in this area. \n#.  Looking at the corresponding polynomial order and depth weight errors \n    versus the reconstruction errors, it can be seen that while it maybe\n    possible to get good weight agreement for a wide range of polynomial-depth configurations\n    these weights may not be capturing all of the salient features of the data. Thus \n    configurations that have poor reconstruction error and good weight error could \n    produce meaningful results for calibrations and VV/UQ. However, these results\n    will only be considering what the latent space was able to capture, and thus \n    may be missing some important parts of the data. \n#.  Using colocating HWD alleviates reconstruction error 'seams' that appear \n    with standard HWD. This is because there is no longer any interpolation or extrapolation\n    between point clouds. All of the data mapping is handled by the GMLS algorithm. The HWD\n    method for this case is only providing data compression which is still useful to avoid \n    memory issues.\n\nBased on these findings, the recommended initial depth for an HWD calibration remains six, with a sixth order polynomial. \nWith these settings its believed that most significant features can be captured and there will be sufficient support \nfor the polynomial pattern functions at that level of subdivisions for most full-field data sets. If there is insufficient \ndata for the recommended HWD configuration, then it is recommended that depth be reduced first before polynomial order. \n\nThese settings are well suited for any mapping problems \nthat will work well for the GMLS mapping algorithm. \nOverall, both methods are very robust which is why\ncolocated HWD is the default HWD method in MatCal. \n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Increasing either HWD parameter\n   will increase run time and memory consumption. It may also result in \n   regions of inadequate support which will result in a failed HWD \n   transformation and errors in the study.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 2"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}