{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Polynomial HWD Verification - Analytical Function\nIn this example, we use an analytical function \nto test and verify our use and implementation of the \nPolynomial HWD algorithm. \nWe designed the verification problem to be \nrepresentative of intended applications and use \ngenerate data with a function that captures much of the \nexpected behavior seen in real data sets.\nWe will compare the HWD weights from two \npoint clouds populated using the function. \nThis weights comparison is analogous to the residual calculation used \nin MatCal for the HWD method where we assume \nthat a minimized difference in the HWD weights for the \ntwo discretizations of the function results \nin a minimized error between the two representations of the \nfunction. To show this, we will also \nquantify the\nerror in an HWD reconstruction of the function against the \nknown values of the function. \nFor this effort, we will \ngenerate two instances\nof our full-field data. One will be the function sampled with \nadded noise which is representative of experimental data.\nThe other \nwill be the same function \nevaluated at different locations without added noise.\nThis is representative \nof the function being generated with a simulation \nwith no model form error.\nWe will complete the following steps for this example:\n\n#.  We evaluate the function on a set of points over a\n    a predetermined domain. This will \n    be referred to as our measurement grid and is\n    meant to be representative of experimental data.\n#.  We add noise with a normal distribution to the \n    data generated in the previous step. The noise \n    has a maximum amplitude of 2.5% of the function \n    maximum value to represent the noise present \n    in measured data.\n#.  We create a separate domain with the same number of points \n    from the measured grid that is unstructured and evaluate the function \n    at these points without noise. This is \n    to be used as the truth \n    value of the function and this set of \n    points will be referred to as the simulation \n    cloud. \n#.  We loop over different input options to the HWD \n    algorithm and evaluate the accuracy of the method\n    against the truth data with five measures of error:\n    (1) the normalized maximum percent error of the weights produced \n    by the HWD tool, (2) the a normalized L2 norm of these \n    weights, (3) the maximum percent error \n    of the function reconstructed on the simulation cloud \n    using the experimental grid HWD weights\n    HWD, (4) the normalized L2 norm \n    of this function and (5) plots of the reconstructed function\n    data error for a subset of the \n    input options studied for the HWD algorithm.\n\nTo begin we import the libraries and tools we will be using to perform this study.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal import *\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rc('text', usetex=True)\nplt.rc('font', family='serif')\nplt.rcParams.update({'font.size': 12})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we defined the domain for our measured grid using NumPy tools.\nThe domain is about 15 mm high (6 inches) and 7.6 mm wide (3 inches). \nThe measured grid has 300 points in each dimension (x, y). \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "H = 6*0.0254\nW = 3*0.0254\n\nmeasured_num_points = 300\nmeasured_xs = np.linspace(-W/2, W/2, measured_num_points)\nmeasured_ys = np.linspace(-H/2, H/2, measured_num_points)\n\nmeasured_x_grid, measured_y_grid = np.meshgrid(measured_xs, measured_ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will define our test function. We are interested \nin generating a function that is representative \nof the full-field data we will use in calibrations \nand other MatCal studies. Generally, these data will be smooth, \nhave some lower frequency and higher frequency behavior \nand may have areas of localized high gradients and values. \nAs a result we choose, the following function which is an additive combination \nof three sinusoids and a linear function that is multiplied \nby a smooth function that approximates a dirac. This \nfunction is defined below: \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def analytical_function(X,Y):\n    small = H/10\n    func = (H/5 * np.sin(np.pi*Y/2/(H/2)) - W/50 * X/(W/2) \n           + H/40*np.sin(np.pi*Y/2/(H/20)) + W/100*np.sin(X/(W/20))) \\\n           * (1+small/(np.pi*(X**2+Y**2+small**2)))\n    return func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now evaluate the function on the measured grid and add\nnoise to it with a maximum amplitude of 2.5% of the maximum \nvalue of the function on the measured grid. We \nthen plot the function with the added noise to verify \nwe are producing the behavior we desire.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measured_func = analytical_function(measured_x_grid, measured_y_grid)\nrng = np.random.default_rng() \nnoise_amp = 0.025*np.max(measured_func)\nnoise_multiplier = rng.random((measured_num_points, measured_num_points)) - .5 \nnoise = noise_multiplier*noise_amp\nmeasured_func += noise\n\nfrom matplotlib import cm\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nax.plot_surface(measured_x_grid, measured_y_grid,  measured_func, \n                cmap=cm.coolwarm)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nax.set_zlabel(\"Z\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the measured data defined, we now create the simulation point cloud\nand the truth data for the simulation point cloud. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim_num_points = measured_num_points\nsim_xs = np.random.uniform(-W/2*1.0, W/2*1.0, sim_num_points**2)\nsim_ys = np.random.uniform(-H/2*1.0, H/2*1.0, sim_num_points**2)\n\nsim_truth_func = analytical_function(sim_xs, sim_ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the measured data and truth simulation data created, \nwe need to prepare the data to be used with the MatCal's\ninterface to the HWD tool. To do so, we create\na :class:`~matcal.full_field.data.FieldData` object for\nboth data sets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measured_dict = {'x':measured_x_grid.reshape(measured_num_points**2), \n                 'y':measured_y_grid.reshape(measured_num_points**2), \n                 'val':measured_func.reshape(1, measured_num_points**2),\n                 'time':np.array([0])}\nmeasured_data = convert_dictionary_to_field_data(measured_dict, \n                                                 coordinate_names=['x','y'])\n\nsim_truth_dict = {'x':sim_xs, \n                  'y':sim_ys, \n                  'val':sim_truth_func.reshape(1, sim_num_points**2), \n                  'time':np.array([0])}\nsim_truth_data = convert_dictionary_to_field_data(sim_truth_dict, \n                                                  coordinate_names=['x','y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create a set of input parameters to \nevaluate using our test data sets. The two input \nparameters to the HWD algorithm are the \npolynomial order of the pattern functions and the depth of subdivision tiers in the splitting tree.\n\nTo study the influence of these parameters on our mapping tool,\nwe perform the mapping with polynomial orders of increasing \npolynomial orders from 1 to 8 and depths of 4 to 10.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "polynomial_orders = np.array([1,2,3,4,6,8], dtype=int) #1,2,3,4,6,8\ncut_depths = np.array([4,6,8,10], dtype=int)#4,6,8,10\nnum_polys = len(polynomial_orders)\nnum_depths = len(cut_depths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then setup a function to compare the HWD weights produced \nfrom the the noisy\nexperimental data to the HWD weights produced from \nthe known truth data on the simulation grid.\nWe will be using the \n:class:`~matcal.full_field.qoi_extractor.HWDPolynomialSimulationSurfaceExtractor`\nclass to perform the HWD operations on point clouds that \nare not collocated. \n\n .. warning::\n  The QoI extractors are not meant for direct use by users. The interfaces will likely \n  change in future releases. Also, the names are specific for their use underneath \n  user facing classes and may not be indicative of how they are used here.\n\nThis function requires the HWD tool input parameters of \npolynomial order and cut depth. It also requires that \ntwo evaluations of the function on the experiment grid\nand on the simulation cloud. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal.full_field.qoi_extractor import HWDPolynomialSimulationSurfaceExtractor\n\ndef get_HWD_results(poly_order, cut_depth, sim_truth_data, measured_data):\n    print(f\"Running Depth {cut_depth}, Order {poly_order}\")\n    hwd_extractor = HWDPolynomialSimulationSurfaceExtractor(sim_truth_data.skeleton, \n                                                            int(cut_depth), int(poly_order), \"time\")\n    measured_weights = hwd_extractor.calculate(measured_data, measured_data, ['val'])            \n    truth_weights = hwd_extractor.calculate(sim_truth_data, measured_data, ['val'])\n    reconstructed_sim = hwd_extractor._hwd._Q.dot(measured_weights['val'])\n    reconstructed_error_field = (reconstructed_sim - sim_truth_data['val'])\n    print(f\"Depth {cut_depth}, Order {poly_order} finished.\")\n    return truth_weights['val'], measured_weights['val'], reconstructed_error_field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can loop over the parameters, generate \nthe HWD basis and store the values \nthat we will be plotting next. These evaluations\nare computationally expensive. As a result, we \nuse Python's ProcessPoolExecutor to \nrun the function in parallel for each \nset of HWD input parameters to speed the calculations.\nWe also store the results in a pickle file so\nthat they are not needlessly recalculated.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_sim_value = np.max(np.abs(sim_truth_data['val']))\nfrom concurrent.futures import ProcessPoolExecutor\nfutures = {}\nwith ProcessPoolExecutor(max_workers = max(num_depths*num_polys, 8)) as executor:    \n    for p_index,poly_order in enumerate(polynomial_orders):\n        futures[poly_order] = {}\n        for d_index, depth in enumerate(cut_depths):\n             futures[poly_order][depth] = get_HWD_results(poly_order, depth, \n                                                          sim_truth_data, measured_data)\n#            futures[poly_order][depth] = executor.submit(get_HWD_results, poly_order, \n#                                                         depth, sim_truth_data, measured_data)           \n\nreconstructed_error_fields = np.zeros((num_polys, num_depths, 1, \n                                       sim_truth_data.spatial_coords.shape[0]))\nall_measured_weights = []\nall_truth_weights = []\nfor p_index,poly_order in enumerate(polynomial_orders):\n    measured_weights_fields_by_depth = []\n    truth_weights_fields_by_depth = []\n    for d_index, depth in enumerate(cut_depths):\n#        results = futures[poly_order][depth].result()\n        results = futures[poly_order][depth]\n        truth_weights_fields_by_depth.append(results[0])\n        measured_weights_fields_by_depth.append(results[1])\n        reconstructed_error_fields[p_index,d_index]  = results[2]          \n    all_measured_weights.append(measured_weights_fields_by_depth)\n    all_truth_weights.append(truth_weights_fields_by_depth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are interested in two error measures. The first \nerror measure we will investigate is the L2-norm \nof the error field normalized by the maximum of the \ntruth data on the simulation cloud multiplied by 100. \nThis is a measure of the general quality of the fit\nfor each point being evaluated and is calculated using\n\n\\begin{align}e_{norm} = \\frac{100}{\\sqrt{m}}  \\frac{\\lVert v_{exp}-v_{sim} \\rVert_2}{\\max\\left(v_{sim}\\right)}\\end{align}\n\nwhere $v_{exp}$ are the values being evaluated that  \nwere generated using the experimental grid points, \n$v_{sim}$ is the known values that \nwere generated at the simulation grid\npoints and $m$ is the number of values generated \nfrom the simulation grid.\nThe second measure of error is the maximum error  \nbetween the values generated from the different \nsources divided by the maximum\nof the truth data and multiplied by 100. This \ngives a maximum percent error for the data \ngenerated from the experiment grid\nrelative to the maximum of the data \ngenerated using the simulation cloud. \nIt is calculated using\n\n\\begin{align}e_{max} = 100\\frac{\\lVert v_{exp}-v_{sim}\\rVert_{\\infty}}{\\max\\left(v_{sim}\\right)}\\end{align}\n\nThese functions are valid for both the HWD weights and function evaluations \ncalculated for\neach discretization.\n\nThe following code performs these calculations and stores the data \nin NumPy arrays so that they can be visualized. It \nalso stores the data in a pickle file so that it can be \nread back later without recalculating since the \ncomputational cost for these calculations can be expensive.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def calculate_error_metrics(measured_fields, truth_fields=None):\n    error_norms = np.zeros((num_polys, num_depths))\n    error_maxes = np.zeros((num_polys, num_depths))\n    for p_index in range(num_polys):\n        for d_index in range(num_depths):\n            if truth_fields:\n                error_vec = (measured_fields[p_index][d_index] - truth_fields[p_index][d_index])\n                val_normalization = np.max(truth_fields[p_index][d_index])\n            else:\n                error_vec = measured_fields[p_index][d_index].flatten()\n                val_normalization = max_sim_value\n            length_normalization = len(error_vec)\n            error_norms[p_index, d_index] = 100 * np.linalg.norm(error_vec) / np.sqrt(length_normalization) / val_normalization\n            error_maxes[p_index, d_index] = 100 * np.max(np.abs(error_vec)) / val_normalization\n    return error_norms, error_maxes\n\nweight_error_norms, weight_error_maxes = calculate_error_metrics(all_measured_weights, all_truth_weights)\nfield_error_norms, field_error_maxes = calculate_error_metrics(reconstructed_error_fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the error fields calculated, we can now create four heat maps \nshowing how our four error measures change as the polynomial order \nand cut depth are varied. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from seaborn import heatmap\nimport matplotlib.colors as colors\n\ndef plot_heatmap(data, title):\n    heatmap(data.T,  annot=True, \n            norm=colors.LogNorm(vmax=1e3),\n            xticklabels=polynomial_orders,\n            yticklabels=cut_depths)\n    plt.title(title)\n    plt.xlabel(\"polynomial order\")\n    plt.ylabel(\"max depth\")\n\nfig = plt.figure(figsize=(10,10), constrained_layout=True)\nax = plt.subplot(2,2,1)\nplot_heatmap(weight_error_norms, \"Weights $e_{{norm}}$\")\nax = plt.subplot(2,2,2)\nplot_heatmap(weight_error_maxes, \"Weights $e_{{max}}$\")\nax = plt.subplot(2,2,3)\nplot_heatmap(field_error_norms, \"Field $e_{{norm}}$\")\nax = plt.subplot(2,2,4)\nplot_heatmap(field_error_maxes, \"Field $e_{{max}}$\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this test, the four error measures \nform a minimum in a diagonal trough from \nthe lower left near depth 10 \nand polynomial order 2 up to the middle right \nwith a depth of 6 and polynomial order of 8.\nThis is highlighting that the example function requires \nminimum level of flexibility in the HWD modes \nto fit the data. It can be achieved with either \nthe level of cuts or the polynomial order for the HWD method.\nWithout enough richness in the basis functions,\nthe HWD method does a poor job representing the space\nand cannot uniquely identify the function \non different discretizations.\nHowever, if there is too much richness as shown in the lower \nright corners of the heat maps, the errors show that the \nsystem is ill-conditioned. This is due to the polynomials \nat the lower length scales are not well supported by the number of points \nincluded in their region of support. \n\nWe now visualize the produced error fields \nover the domain of interest for the polynomial orders of three to six \nand cut depths of six to ten.  \nWe look at the error fields for these \ninputs to the HWD tools because most \nof them provide good agreement for the HWD weight error\nmeasures. The one that does not have low HWD weight \nerror measures is the evaluation with a depth of ten \nand a polynomial order of six.\nIt is shown to highlight some of the potential issues \nto be wary of with high depth cuts and high polynomials.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "poly_start_index = 2\ndepth_start_index = 1\nviewed_polys = polynomial_orders[poly_start_index:-1]\nviewed_depths =  cut_depths[depth_start_index:]\nfig, ax_set = plt.subplots(len(viewed_polys), len(viewed_depths),\n                            figsize=(5*len(viewed_depths), 5*len(viewed_polys)))\nfor row, po in enumerate(viewed_polys):\n    ax_set[row,0].set_ylabel(f\"Order: {po}\")\n    for col, depth in enumerate(viewed_depths):                \n        ax = ax_set[row, col]\n        if row == 0:\n            ax.set_title(f\"Depth: {depth}\")\n        error_field = reconstructed_error_fields[row+poly_start_index, \n                                                 col+depth_start_index]\n        error_field = np.abs(error_field/max_sim_value*100)\n        cs = ax.scatter(sim_xs, sim_ys, c=error_field.flatten(), \n                        norm = colors.LogNorm(vmin=1e-2, vmax=1e1), \n                        cmap='magma', marker='.', s=.9)\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        ax.set_xticks([])\n        ax.set_yticks([])\nfig.colorbar(cs, ax=ax_set.ravel())\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From these plots, the following conclusions can be made: \n\n#.  Recreation error is highest in the central peak region. \n    Increasing polynomial order and depth better characterized \n    the local behavior at this location. Increasing the depth \n    of HWD allowed for more support of the central region. \n    Increasing the polynomial order added additional flexibility \n    to the wave forms allowing for a more accurate reconstruction in this area. \n#.  Looking at the corresponding polynomial order and depth weight errors \n    versus the reconstruction errors, it can be seen that while it maybe\n    possible to get good weight agreement for a wide range of polynomial-depth configurations\n    these weights may not be capturing all of the salient features of the data. Thus \n    configurations that have poor reconstruction error and good weight error could \n    produce meaningful results for calibrations and VV/UQ. However, these results\n    will only be considering what the latent space was able to capture, and thus \n    may be missing some important parts of the data. \n#.  The subdivision selection used in HWD misses important aspects of the data. \n    In the reconstruction error 'seams' can be seen that indicate the different subdivisions \n    created by HWD. These do not seem to be arranged in a fashion that would allow the pattern \n    functions to create the best basis possible. This is to be expected because of the purely \n    geometry based decomposition method existing within the HWD library. \n\nBased on these findings, the recommended initial depth for an HWD calibration is six, with a sixth order polynomial. \nWith these settings its believed that most significant features can be captured and there will be sufficient support \nfor the polynomial pattern functions at that level of subdivisions for most full-field data sets. If there is insufficient \ndata for the recommended HWD configuration, then it is recommended that depth be reduced first before polynomial order. \n\nThese settings are best suited for mapping problems with the following \ncharacteristics:\n\n#. The underlying function being studied is relatively smooth \n   when compared to the discretization point cloud spacing. In \n   other words, the point cloud spacing should be significantly\n   smaller than the size of the features of interest for the function\n   that they hold data for.\n#. The data being compared is not extrapolated. The higher order polynomials\n   and small areas of support will lead to large extrapolation errors. \n\nIf the data set is not smooth, then higher order polynomials may create a worse representation of the data. \nIn these cases it is better to reduce the polynomial orders used and increase the depth of the HWD tree. This\nwill allow the representation to better align with rough or discontinuous data. In addition, while HWD has \ngreat potential to capture discontinuous data patterns, it does this best when subdivision lines coexist with \nregions of discontinuous behavior. Improving the geometric decomposition of the domain is planned \nplanned for future releases.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Increasing either HWD parameter\n   will increase run time and memory consumption. It may also result in \n   regions of inadequate support which will result in a failed HWD \n   transformation and errors in the study.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 2"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}