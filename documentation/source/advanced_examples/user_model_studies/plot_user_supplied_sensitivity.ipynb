{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Latin Hypercube Sampling to Obtain Local Material Sensitivities\nIn this section, we will cover how to run a Latin Hypercube Sampling study using\na model from an external \nphysics modeling software. MatCal is written to be modular, so any tools taught \nhere can be mixed and matched with tools presented in other examples. \nThis example builds on the work done in the previous section, `Calibration \nof Two Different Material Conductivities`. The points discussed in \nthis example will be used to elucidate the differences in setup between a \ncalibration study and a sensitivity study. \n\nIf you want more details about particular types of studies please see \n`MatCal Studies`. \n\nAfter performing our calibration in the previous example, we want to know \nhow important our \nconductivities are to our prediction. To do this, we want to \nget a measure of our solution's \nsensitivity to our parameters. Fortunately for us, we have done \nmuch of the hard work already \nfor the calibration, and we can just change a few lines in our \nMatCal script to perform a Latin Hypercube Sampling study \nand get Pearson correlation values. Pearson correlations tell \nus how correlated our parameters are with our quantities of interest. \nFor this study, we can begin by adapting our MatCal input from \nthe previous example. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal import *\nimport numpy as np\n\ncond_1 = Parameter(\"K_foam\", .1 , .2, distribution=\"uniform_uncertain\")\ncond_2 = Parameter(\"K_steel\", 30, 60, distribution=\"uniform_uncertain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start our MatCal input the same way we did with the calibration, \nby importing MatCal and defining our \nparameters. However, we are only attempting to asses how sensitive our model \nis to the parameters over the range of interest. As a result, data do \nnot need to be supplied. We just need to define what fields we are interested\nin from the model results and at what independent values and states we want\nthese data. To that end, we create the \nstate of interest and independent values of interest below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "low_flux_state = State(\"LowFlux\", exp_flux=1e3)\nimport numpy as np\nindependent_field_values = np.linspace(0,20,11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The objective for this study will be a \n:class:`~matcal.core.objective.SimulationResultsSynchronizer`.\nSince we do not need to compare to data for this study, \nthis is will synchronize simulation results to common \nindependent field values that are user specified for comparison.\nFor MatCal, the synchronizer behaves like an objective and \nis comparing the simulation \nresults to a vector of zeros for each dependent field. \nAs a result, there is no data conditioning, normalization or weighting\napplied to the simulation results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "objective = SimulationResultsSynchronizer(\"time\", independent_field_values,\n                                          \"T_middle\", \"T_bottom\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we set up our model and objective just as we \ndid in the calibration example. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "user_file = \"two_material_square.i\"\ngeo_file = \"two_material_square.g\"\nsim_results_file = \"two_material_results.csv\"\nmodel = UserDefinedSierraModel('aria', user_file, geo_file)\nmodel.set_results_filename(sim_results_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last difference between the calibration study and \nthis sensitivity study is our choice of \nstudy. In this example we are using a \n:class:`~matcal.dakota.sensitivity_studies.LhsSensitivityStudy`. \nWe initialize it and add our evaluation set\nto it as is common MatCal study procedure.\nIn this study we are just looking at one state. \nHowever,  multiple states can be run if desired through \nthe ``states`` keyword argument. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sens = LhsSensitivityStudy(cond_1, cond_2)\nsens.add_evaluation_set(model, objective, states=low_flux_state)\nsens.set_core_limit(56)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last input needed is how many samples to take in the LHS study. \nThe study needs a certain number of samples to \nproduce a converged solution; however, that number  \nis problem dependent. We will likely have to run our study a \nfew times to confirm we have a converged solution. \nA decent starting guess is ten times the number of parameters \nyou are studying. As a result,\nwe set the number of samples to 20. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sens.set_number_of_samples(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now all that is left to do is to launch the study and wait for our results. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = sens.launch()\nprint(results)\nmake_standard_plots('time')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now repeat the study, but request Sobol Indices to \nbe output. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sens = LhsSensitivityStudy(cond_1, cond_2)\nsens.set_random_seed(1702)\nsens.add_evaluation_set(model, objective, states=low_flux_state)\nsens.set_core_limit(56)\nsens.set_number_of_samples(20)\nsens.make_sobol_index_study()\nresults = sens.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that much more examples are \nnow run. For a study producing Sobol indices,\nDakota will run $N*(M+2)$ samples \nwhere $N$ is the number of requested samples \nand $M$ is the number of study parameters in the study.\nAs a result, for this study a total of 80 samples are run.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can bee seen above, there are some unexpected results. The \nmethod provides the sensitivity indices for main effects and the \ntotal effects for each parameter, respectively. The main \neffects are representative of the contribution of each \nstudy parameter the variance in the model response.\nWhile the total effects represent the contribution \nof each parameter in combination with all \nthe other parameters to the variance in the model response.\n\nFor both cases the result should be positive. However, \nin these results the ``K_steel`` parameter has \nsome negative values. This is most likely \ndue to the sampling size being too small and the fact \nthat the index values are small. As a result, \nnumerical errors cause the values to become negative.\nTo investigate this issue we \nre-run the study with more samples. This time \nwe choose a sample size of 200, which will run \na total of 800 models. Dakota's documentation \nrecommends hundreds to thousands of samples for a\nsampling study producing Sobol indices. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sens = LhsSensitivityStudy(cond_1, cond_2)\nsens.add_evaluation_set(model, objective, states=low_flux_state)\nsens.set_core_limit(56)\nsens.set_random_seed(1702)\nsens.set_number_of_samples(200)\nsens.make_sobol_index_study()\nresults = sens.launch()\nprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In these results, we see that all of the indices have changed\nsignificantly. \nThis indicates that the Sobol indices are likely not converged.\nFor a real problem, users should continue running studies with\nincreasing samples \nuntil the indices converge. Also, regarding the negative values \nfrom the 20 sample study, some values are still negative. \nThis is likely due to them being near zero and within \nexpected numerical errors with the number of samples.\nIf we were to do a proper sample size convergence study, \nthese would continue decreasing in magnitude but may \nnever turn all positive.  Although potentially not converged, \nwe can plot the current results and make conclusions about the \ninfluence of the parameters on our QoIs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "make_standard_plots('time')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In these results, we see that across all time the conductivity of\nthe foam has a strong correlation with both of our\ntemperature values of interest,\nwhile the conductivity of steel has very little. \nThis indicates that this experimental series is likely a good \napproach for determining a foam conductivity, \nhowever, is less useful in determining the steel conductivity. \nIt would be useful to find\nanother set of data to help us study the steel. \n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}