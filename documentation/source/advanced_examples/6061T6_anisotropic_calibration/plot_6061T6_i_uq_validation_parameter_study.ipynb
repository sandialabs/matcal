{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 6061T6 aluminum uncertainty quantification validation\nIn this example, we will use MatCal's :class:`~matcal.core.parameter_studies.ParameterStudy`\nto validate the estimated parameter uncertainty for the calibration. \nWe do this by generating samples from the fitted covariance from \n`6061T6 aluminum calibration with anisotropic yield` and \nrunning the calibrated models with these samples. Then the \nmodel results are compared to the data to see how well the sampled parameter \nsets allow the models to represent the data uncertainty. \n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Useful Documentation links:\n\n    #. `Uniaxial Tension Models`\n    #. `Top Hat Shear Model`\n    #. :class:`~matcal.core.parameter_studies.ParameterStudy`\n    #. :func:`~matcal.core.parameter_studies.sample_multivariate_normal`</p></div>\nTo begin, we import the tools we need for this study and setup the \ndata and model as we did in `6061T6 aluminum calibration with anisotropic yield`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matcal import *\nfrom matcal.sandia.computing_platforms import is_sandia_cluster, get_sandia_computing_platform\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rc('text', usetex=True)\nplt.rc('font', family='serif')\nplt.rc('font', size=12)\nfigsize = (4,3)\n\ntension_data_collection = BatchDataImporter(\"aluminum_6061_data/\" \n                                              \"uniaxial_tension/processed_data/\"\n                                              \"cleaned_[CANM]*.csv\",).batch\n\ndown_selected_tension_data = DataCollection(\"down selected data\")\nfor state in tension_data_collection.keys():\n    for index, data in enumerate(tension_data_collection[state]):\n        stresses = data[\"engineering_stress\"]\n        strains = data[\"engineering_strain\"]    \n        peak_index = np.argmax(stresses)\n        peak_strain = strains[peak_index]\n        peak_stress = stresses[peak_index]\n        data_to_keep = (((strains>peak_strain) & (stresses > 0.89*peak_stress)) | \n                        (strains>0.005) & (strains < peak_strain))\n        down_selected_tension_data.add(data[data_to_keep])\n\ndown_selected_tension_data = scale_data_collection(down_selected_tension_data, \n                                                   \"engineering_stress\", 1000)\ndown_selected_tension_data.remove_field(\"time\")\n\nmaterial_filename = \"hill_plasticity_temperature_dependent.inc\"\nmaterial_model = \"hill_plasticity\"\nmaterial_name = \"ductile_failure_6061T6\"\nsierra_material = Material(material_name, material_filename, material_model)\n\ngauge_radius = 0.125\nelement_size = gauge_radius/8\ngeo_params = {\"extensometer_length\": 1.0,\n              \"gauge_length\": 1.25,\n              \"gauge_radius\": gauge_radius,\n              \"grip_radius\": 0.25,\n              \"total_length\": 4,\n              \"fillet_radius\": 0.188,\n              \"taper\": 0.0015,\n              \"necking_region\":0.375,\n              \"element_size\": element_size,\n              \"mesh_method\":3,\n              \"grip_contact_length\":1}\n\ntension_model = RoundUniaxialTensionModel(sierra_material, **geo_params)            \ntension_model.set_name(\"tension_model\")\ntension_model.add_boundary_condition_data(down_selected_tension_data)\ntension_model.set_allowable_load_drop_factor(0.70)\ntension_model.set_boundary_condition_scale_factor(1.5)\n\nmy_wcid = \"fy220213\"\nif is_sandia_cluster():\n  tension_model.run_in_queue(my_wcid, 1)\n  tension_model.continue_when_simulation_fails()\n  platform = get_sandia_computing_platform()\n  num_cores = platform.get_processors_per_node()\nelse:\n  num_cores = 8\ntension_model.set_number_of_cores(num_cores)\n\ntop_hat_data_collection = BatchDataImporter(\"aluminum_6061_data/\" \n                                              \"top_hat_shear/processed_data/cleaned_*.csv\").batch\nfor state, state_data_list in top_hat_data_collection.items():\n    for index, data in enumerate(state_data_list):\n        max_load_arg = np.argmax(data[\"load\"])\n        data = data[data[\"time\"] < data[\"time\"][max_load_arg]]\n        data = data[data[\"load\"] > 0.005]\n        top_hat_data_collection[state][index] = data[data[\"displacement\"] < 0.02]\ntop_hat_data_collection.remove_field(\"time\")\n\ntop_hat_geo_params = {\"total_height\":1.25,\n        \"base_height\":0.75,\n        \"trapezoid_angle\": 10.0,\n        \"top_width\": 0.417*2,\n        \"base_width\": 1.625, \n        \"base_bottom_height\": (0.75-0.425),\n        \"thickness\":0.375, \n        \"external_radius\": 0.05,\n        \"internal_radius\": 0.05,\n        \"hole_height\": 0.3,\n        \"lower_radius_center_width\":0.390*2,\n        \"localization_region_scale\":0.0,\n        \"element_size\":0.005, \n        \"numsplits\":1}\n\ntop_hat_model = TopHatShearModel(sierra_material, **top_hat_geo_params)\ntop_hat_model.set_name('top_hat_shear')\ntop_hat_model.set_allowable_load_drop_factor(0.05)\ntop_hat_model.add_boundary_condition_data(top_hat_data_collection)\ntop_hat_model.set_number_of_cores(num_cores*2)\nif is_sandia_cluster():\n  top_hat_model.run_in_queue(my_wcid, 1)\n  top_hat_model.continue_when_simulation_fails()\n\ntension_objective = CurveBasedInterpolatedObjective(\"engineering_strain\", \"engineering_stress\")\ntension_objective.set_name(\"engineering_stress_strain_obj\")\ntop_hat_objective = CurveBasedInterpolatedObjective(\"displacement\", \"load\")\ntop_hat_objective.set_name(\"load_displacement_obj\")\n\nRT_calibrated_params = matcal_load(\"anisotropy_parameters.serialized\")\nyield_stress = Parameter(\"yield_stress\", 15, 50, \n        RT_calibrated_params.pop(\"yield_stress\"))\nhardening = Parameter(\"hardening\", 0, 60, \n        RT_calibrated_params.pop(\"hardening\"))\nb = Parameter(\"b\", 10, 40,\n        RT_calibrated_params.pop(\"b\"))\nR22 = Parameter(\"R22\", 0.8, 1.15, \n        RT_calibrated_params[\"R22\"])\nR33 = Parameter(\"R33\", 0.8, 1.15, \n        RT_calibrated_params[\"R33\"])\nR12 = Parameter(\"R12\", 0.8, 1.15, \n        RT_calibrated_params[\"R12\"])\nR23 = Parameter(\"R23\", 0.8, 1.15, \n        RT_calibrated_params[\"R23\"])\nR31 = Parameter(\"R31\", 0.8, 1.15,\n        RT_calibrated_params[\"R31\"])\n\npc = ParameterCollection(\"uncertain_params\", yield_stress, hardening, b)\n\nhigh_temp_calibrated_params = matcal_load(\"temperature_dependent_parameters.serialized\")\ntension_model.add_constants(**high_temp_calibrated_params,\n                            **RT_calibrated_params)\ntop_hat_model.add_constants(**high_temp_calibrated_params,\n                            **RT_calibrated_params)\n\nresults = matcal_load(\"laplace_study_results.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After importing laplace study results, we can \nsample parameters sets from the estimated parameter\nuncertainties using :func:`~matcal.core.parameter_studies.sample_multivariate_normal`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_samples = 50\nuncertain_parameter_sets = sample_multivariate_normal(num_samples, \n                                                      results.mean.to_list(),\n                                                      results.fitted_parameter_covariance, \n                                                      seed=1234, \n                                                      param_names=pc.get_item_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we set up a study so we can \nvisualize the results by pushing the samples back through the models.\nWe do so using a MatCal :class:`~matcal.core.parameter_studies.ParameterStudy`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "param_study = ParameterStudy(pc)\nparam_study.add_evaluation_set(tension_model, tension_objective, down_selected_tension_data)\nparam_study.add_evaluation_set(top_hat_model, top_hat_objective, top_hat_data_collection)\nparam_study.set_core_limit(250)\nparam_study.set_working_directory(\"UQ_sampling_study\", remove_existing=True)\nparams_to_evaluate = zip(uncertain_parameter_sets[\"yield_stress\"],\n                         uncertain_parameter_sets[\"hardening\"],\n                         uncertain_parameter_sets[\"b\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we add parameter evaluations for each of the samples. \nWe do so by organizing the data using Python's\n``zip`` function and then loop over the result\nto add each parameter set sample to the study.\n\n.. Warning::\n   We add error catching to the addition of each parameter \n   evaluation. There is a chance that parameters could be \n   generated outside of our original bounds and we want the study to complete.\n   If this error is caught, we will see it in the MatCal output \n   and know changes are needed. However, some results will still be output\n   and can be of use.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "valid_runs = 0\nfor params in params_to_evaluate:\n    y_eval    = params[0]\n    A_eval    = params[1]\n    b_eval    = params[2]\n \n    try:\n      param_study.add_parameter_evaluation(yield_stress=y_eval, hardening=A_eval,b=b_eval)\n      print(f\"Running evaluation {params}\")\n      valid_runs +=1                         \n    except ValueError:\n       print(f\"Skipping evaluation with {params}. Parameters out of range. \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we launch the study and plot the results.\nWe use functions to simplify the plotting processes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if valid_runs > 0:\n    param_study_results = param_study.launch()\nelse:\n    exit()\n\ndef compare_data_and_model(data, model_responses, indep_var, dep_var, \n                           plt_func=plt.plot, fig_label=None):\n    if fig_label is not None:\n        fig = plt.figure(fig_label)\n    else:\n        fig = None\n    data.plot(indep_var, dep_var, plot_function=plt_func, ms=3, labels=\"data\", \n            figure=fig, marker='o', linestyle='-', color=\"#bdbdbd\", show=False)\n    model_responses.plot(indep_var, dep_var, plot_function=plt_func,labels=\"models\", \n                      figure=fig, linestyle='-', alpha=0.5)\n\nall_tension_data = tension_data_collection\nall_tension_data = scale_data_collection(all_tension_data, \n                                                  \"engineering_stress\", 1000)\nall_sim_tension_data = param_study_results.simulation_history[tension_model.name]\ncompare_data_and_model(all_tension_data, \n                       all_sim_tension_data, \n                       \"engineering_strain\", \"engineering_stress\")\n\nall_top_hat_sim_data =param_study_results.simulation_history[top_hat_model.name]\ncompare_data_and_model(top_hat_data_collection, \n                       all_top_hat_sim_data, \n                       \"displacement\", \"load\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the plots, the simulation results for the simulated samples\ndoes not match the variation in the \ndata sets in the areas where the data were used for calibration, and \nseem to be a poor representation of the uncertainty. Also,\nmany of the parameter samples were rejected due to being out of bounds indicating\nan unacceptable results.\nA potential alternative uncertainty quantification option, \nthat is more computationally expensive, is to do data resampling. With data resampling, \nrandom data sets for each model are chosen and the models are calibrated to this\nrandom selection. This is repeated for many sample selections. After many calibrations\nare completed, a population of valid parameter sets are obtained and can be used \nas the uncertain parameter distributions for the parameters.  \n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}